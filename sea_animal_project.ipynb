{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+x5+gNSzVF9fgrZEjkNf3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/garylau1/model_training/blob/main/sea_animal_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction\n",
        "The diversity of marine life presents a unique opportunity to leverage deep learning for the classification and identification of sea animals. In this project, I developed a robust machine learning pipeline to classify 23 distinct species of sea animals using a pre-trained Wide ResNet-50-2 architecture. This architecture, renowned for its superior feature extraction capabilities and computational efficiency, served as the backbone of the model, allowing for accurate classification in a complex and nuanced dataset.\n",
        "\n",
        "The model achieved an impressive accuracy exceeding 80%, demonstrating its effectiveness in discerning subtle differences among species. Furthermore, to ensure accessibility and scalability, the trained model was deployed on the Hugging Face platform, enabling seamless integration and real-world application. This report outlines the methodology, implementation, and results of the project, highlighting the significant role of pre-trained architectures in advancing marine biodiversity research.\n",
        "\n",
        "\n",
        "Pre-train model link: https://pytorch.org/vision/main/models/generated/torchvision.models.wide_resnet50_2.html#torchvision.models.Wide_ResNet50_2_Weights\n",
        "\n"
      ],
      "metadata": {
        "id": "LJhkhtG1_YSL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEhDvSLXwWIU",
        "outputId": "32e0b552-4a83-468f-82f9-720371703ec8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch  # Core library for PyTorch\n",
        "import torchvision  # PyTorch library for vision-related tasks\n",
        "import random  # Used for generating random values\n",
        "import matplotlib.pyplot as plt  # For visualizing images and results\n",
        "\n",
        "# Additional imports\n",
        "from torch import nn  # Neural network modules in PyTorch\n",
        "from torchvision import transforms  # For applying data transformations\n",
        "\n",
        "# File handling and image processing\n",
        "import requests  # For downloading data from the internet\n",
        "import zipfile  # For extracting .zip files\n",
        "from pathlib import Path  # For file path operations\n",
        "from PIL import Image  # For working with image files\n",
        "\n",
        "# Dataset handling\n",
        "from torchvision import datasets  # Prebuilt datasets in torchvision\n",
        "from torch.utils.data import Dataset, DataLoader  # Dataset and data loader utilities\n",
        "import os  # OS module for file and directory operations\n",
        "\n",
        "# Setting the device for computation\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Use GPU if available, otherwise fallback to CPU\n",
        "\n",
        "# 1. Import libraries for data acquisition and preparation\n",
        "from pathlib import Path  # Helps in working with file paths\n",
        "import requests  # Used to download files from the internet\n",
        "import zipfile  # For extracting compressed files\n",
        "\n",
        "# Mount Google Drive (specific to Google Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  # Mounts the Google Drive to access datasets and save outputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the target path for the dataset\n",
        "target_path = Path(\"Our_datata\")  # Specify the folder to store the dataset\n",
        "\n",
        "# Check if the target path exists\n",
        "if not os.path.exists(target_path):  # If the folder doesn't exist,\n",
        "    os.mkdir(target_path)  # Create the folder\n",
        "\n",
        "import shutil  # Import shutil for file operations\n",
        "\n",
        "# Define the source and destination paths\n",
        "source_path = '/content/drive/MyDrive/sea_animal.zip'  # Path to the zip file in Google Drive\n",
        "\n",
        "# Copy the zip file from the source path to the target folder\n",
        "shutil.copy(source_path, target_path)\n",
        "\n",
        "# Unpack (extract) the zip file into the target folder\n",
        "shutil.unpack_archive(\"/content/Our_datata/sea_animal.zip\", target_path)\n",
        "\n",
        "# Remove the zip file after extraction to save space\n",
        "os.remove(\"/content/Our_datata/sea_animal.zip\")\n",
        "\n"
      ],
      "metadata": {
        "id": "lcFMc0S8xhLo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretrained Model:\n",
        "This section defines a function to create a classification model based on the Wide ResNet-50-2 architecture.\n",
        "\n",
        "The function modifies the pre-trained model to freeze its existing weights and replaces the final layer to adapt to\n",
        "\n",
        "\n",
        "the task of classifying 23 sea animal species. The model also includes data transformations specific to the pre-trained weights."
      ],
      "metadata": {
        "id": "mfsTRMZ3Clum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def created_model(class_n=23):\n",
        "    \"\"\"\n",
        "    Creates and customizes a Wide ResNet-50-2 model for multi-class classification.\n",
        "\n",
        "    Args:\n",
        "        class_n (int): Number of output classes for the model. Default is 23.\n",
        "\n",
        "    Returns:\n",
        "        model (torch.nn.Module): The modified Wide ResNet-50-2 model.\n",
        "        transform_ (callable): Pre-defined data transformations for the model.\n",
        "    \"\"\"\n",
        "    # Load pre-trained weights for Wide ResNet-50-2\n",
        "    weights = torchvision.models.Wide_ResNet50_2_Weights.DEFAULT\n",
        "\n",
        "    # Initialize the pre-trained Wide ResNet-50-2 model\n",
        "    model = torchvision.models.wide_resnet50_2(weights)\n",
        "\n",
        "    # Get the transformation associated with the pre-trained weights\n",
        "    transform_ = weights.transforms()\n",
        "\n",
        "    # Freeze all layers of the model to prevent updates during training\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Replace the final fully connected (fc) layer with a custom classifier\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Dropout(p=0.4, inplace=True),  # Add dropout for regularization\n",
        "        nn.Linear(model.fc.in_features, model.fc.in_features),  # Intermediate fully connected layer\n",
        "        nn.ReLU(),  # Activation function\n",
        "        nn.Linear(model.fc.in_features, out_features=class_n)  # Final output layer with 'class_n' outputs\n",
        "    )\n",
        "\n",
        "    # Return the customized model and the corresponding data transformation\n",
        "    return model, transform_\n",
        "\n",
        "# Create a new instance of the model and its transformation for 23 classes\n",
        "new_model, transform_ = created_model(23)\n",
        "\n"
      ],
      "metadata": {
        "id": "Xvj5jvhUw3uU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Splitting and DataLoader Setup\n",
        "This section splits the dataset into training and testing subsets, with 85% of data used for training and 15% for testing. It then creates DataLoader instances for both the training and testing datasets, which will handle batching, shuffling, and parallel data loading during training and evaluation.\n",
        "\n",
        "Finally, an Adam optimizer and a Cross-Entropy loss function with label smoothing are set up for the model.\n"
      ],
      "metadata": {
        "id": "DdpPrfH3DSeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine additional transformations with the pre-trained model's specific transformation\n",
        "final_transform = transforms.Compose([\n",
        "    transforms.TrivialAugmentWide(),  # Apply a wide range of simple augmentations for data diversity\n",
        "    transform_  # Include the pre-defined transformation from the pre-trained model weights\n",
        "])\n",
        "\n",
        "# Load the dataset using ImageFolder\n",
        "Dataset_ = datasets.ImageFolder(\n",
        "    root=\"/content/Our_datata\",  # Path to the root directory containing the dataset\n",
        "    transform=final_transform  # Apply the composed transformations to the images\n",
        ")\n",
        "#TypeError: Only torch.uint8 image tensors are supported, but found torch.float32\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "# 85% for training and 15% for testing\n",
        "train_dataset, test_dataset = random_split(Dataset_, [0.85, 0.15])\n",
        "\n",
        "# Import DataLoader to load the data in batches\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create DataLoader for the training dataset\n",
        "# Batch size of 16, shuffling enabled to improve training, and using 2 workers for parallel data loading\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
        "\n",
        "# Create DataLoader for the testing dataset\n",
        "# Batch size of 16, no shuffling as test data should not be randomized, and using 2 workers\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "# Initialize the Adam optimizer for the model with learning rate of 0.001\n",
        "optimizer = torch.optim.Adam(new_model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the loss function to be used during training (Cross-Entropy Loss with label smoothing)\n",
        "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)"
      ],
      "metadata": {
        "id": "pJk0VtLM_k2A"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Model Training\n",
        "\n",
        "This function model_training trains the model using the training data, evaluates it on the test data, and returns the average loss and accuracy for both the training and testing phases. The model, loss function, optimizer, and dataloaders are passed as arguments, and the function runs the training and evaluation loop over the dataset for one epoch.\n"
      ],
      "metadata": {
        "id": "XU6I1o1ADlSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_training(model=new_model, train_dataloader=train_dataloader, test_dataloader=test_dataloader, optimizer=optimizer, loss_fn=loss_fn, device=device):\n",
        "    \"\"\"\n",
        "    Trains the given model on the training dataset and evaluates it on the test dataset.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to be trained.\n",
        "        train_dataloader (torch.utils.data.DataLoader): The DataLoader for the training dataset.\n",
        "        test_dataloader (torch.utils.data.DataLoader): The DataLoader for the testing dataset.\n",
        "        optimizer (torch.optim.Optimizer): The optimizer used for training.\n",
        "        loss_fn (torch.nn.Module): The loss function used during training.\n",
        "        device (str): The device ('cuda' or 'cpu') where the model and data should be loaded.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the average training loss, average training accuracy, average test loss, and average test accuracy.\n",
        "    \"\"\"\n",
        "\n",
        "    # Move the model to the specified device (GPU/CPU)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Set the model in training mode\n",
        "    model.train()\n",
        "\n",
        "    # Initialize variables to track the total training loss and accuracy\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "\n",
        "    # Initialize variables to track the total test loss and accuracy\n",
        "    test_loss = 0\n",
        "    test_acc = 0\n",
        "\n",
        "    # Training loop over the training dataset\n",
        "    for batch, (X, y) in enumerate(train_dataloader):\n",
        "        # Move inputs and targets to the correct device\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Forward pass: Compute predicted y by passing X to the model\n",
        "        y_pred = model(X)\n",
        "\n",
        "        # Compute the loss between predictions and ground truth labels\n",
        "        loss = loss_fn(y_pred, y)\n",
        "\n",
        "        # Calculate accuracy by comparing the predicted labels to the true labels\n",
        "        acc = (sum(y_pred.argmax(1) == y).item()) / len(y)\n",
        "\n",
        "        # Accumulate the loss and accuracy for training\n",
        "        train_acc += acc\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Zero the gradients before performing the backward pass\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass: Compute gradients of the loss w.r.t the model parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Update model parameters using the optimizer\n",
        "        optimizer.step()\n",
        "\n",
        "    # Set the model in evaluation mode (no gradient updates)\n",
        "    model.eval()\n",
        "\n",
        "    # No gradient tracking needed for inference\n",
        "    with torch.inference_mode():\n",
        "        # Evaluation loop over the test dataset\n",
        "        for batch, (X, y) in enumerate(test_dataloader):\n",
        "            # Move inputs and targets to the correct device\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # Forward pass: Compute predicted y by passing X to the model\n",
        "            y_pred = model(X)\n",
        "\n",
        "            # Compute the loss between predictions and ground truth labels\n",
        "            loss = loss_fn(y_pred, y)\n",
        "\n",
        "            # Calculate accuracy by comparing the predicted labels to the true labels\n",
        "            acc = (sum(y_pred.argmax(1) == y).item()) / len(y)\n",
        "\n",
        "            # Accumulate the loss and accuracy for testing\n",
        "            test_acc += acc\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    # Return the average loss and accuracy for both training and testing\n",
        "    return train_loss / len(train_dataloader), train_acc / len(train_dataloader), test_loss / len(test_dataloader), test_acc / len(test_dataloader)\n",
        "\n"
      ],
      "metadata": {
        "id": "jxozmMWpCwyB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We store our result:\n",
        "Result={\"train_loss\":[],\"train_acc\":[],\"test_loss\":[],\"test_acc\":[]}\n",
        "for i in range(30):\n",
        "  train_loss,train_acc,test_loss,test_acc=model_training()\n",
        "  print(f\"train_loss:{train_loss:5f} |  train_acc:{train_acc:5f} |  test_loss:{test_loss:5f} | test_acc:{test_acc:5f}\")\n",
        "  Result[\"train_loss\"].append(train_loss)\n",
        "  Result[\"train_acc\"].append(train_acc)\n",
        "  Result[\"test_loss\"].append(test_loss)\n",
        "  Result[\"test_acc\"].append(test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "id": "__W5LqXIyDLg",
        "outputId": "44314e46-b4ab-4a93-ec58-e3c323b78631"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss:1.058358 |  train_acc:0.859225 |  test_loss:1.165916 | test_acc:0.831880\n",
            "train_loss:1.063266 |  train_acc:0.859262 |  test_loss:1.156632 | test_acc:0.832849\n",
            "train_loss:1.053143 |  train_acc:0.868031 |  test_loss:1.120863 | test_acc:0.840116\n",
            "train_loss:1.063431 |  train_acc:0.856837 |  test_loss:1.147990 | test_acc:0.828488\n",
            "train_loss:1.047792 |  train_acc:0.866231 |  test_loss:1.170182 | test_acc:0.823643\n",
            "train_loss:1.061826 |  train_acc:0.860976 |  test_loss:1.140749 | test_acc:0.833818\n",
            "train_loss:1.056756 |  train_acc:0.863977 |  test_loss:1.161019 | test_acc:0.825097\n",
            "train_loss:1.056866 |  train_acc:0.859801 |  test_loss:1.157367 | test_acc:0.829942\n",
            "train_loss:1.034715 |  train_acc:0.872893 |  test_loss:1.142809 | test_acc:0.832364\n",
            "train_loss:1.039540 |  train_acc:0.870407 |  test_loss:1.160575 | test_acc:0.832364\n",
            "train_loss:1.033799 |  train_acc:0.869660 |  test_loss:1.172807 | test_acc:0.822674\n",
            "train_loss:1.036714 |  train_acc:0.871044 |  test_loss:1.145317 | test_acc:0.829457\n",
            "train_loss:1.022129 |  train_acc:0.876923 |  test_loss:1.165528 | test_acc:0.822674\n",
            "train_loss:1.039013 |  train_acc:0.871032 |  test_loss:1.166085 | test_acc:0.821705\n",
            "train_loss:1.022133 |  train_acc:0.878209 |  test_loss:1.151927 | test_acc:0.826550\n",
            "train_loss:1.026270 |  train_acc:0.874486 |  test_loss:1.144808 | test_acc:0.831880\n",
            "train_loss:1.026154 |  train_acc:0.871179 |  test_loss:1.148759 | test_acc:0.836240\n",
            "train_loss:1.014493 |  train_acc:0.879348 |  test_loss:1.151216 | test_acc:0.833333\n",
            "train_loss:1.014258 |  train_acc:0.880891 |  test_loss:1.131284 | test_acc:0.830426\n",
            "train_loss:1.005005 |  train_acc:0.884578 |  test_loss:1.149753 | test_acc:0.824128\n",
            "train_loss:1.012802 |  train_acc:0.878405 |  test_loss:1.146722 | test_acc:0.829457\n",
            "train_loss:1.005516 |  train_acc:0.885006 |  test_loss:1.143312 | test_acc:0.832364\n",
            "train_loss:1.004804 |  train_acc:0.883696 |  test_loss:1.153572 | test_acc:0.828488\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-1f0acf692caf>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mResult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"train_loss\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"train_acc\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"test_loss\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"test_acc\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"train_loss:{train_loss:5f} |  train_acc:{train_acc:5f} |  test_loss:{test_loss:5f} | test_acc:{test_acc:5f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mResult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-0bd0446010cf>\u001b[0m in \u001b[0;36mmodel_training\u001b[0;34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0macc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mtest_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtest_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion of Results\n",
        "\n",
        "The training and testing results presented above reflect the performance of the sea animal classification model, trained using the pre-trained Wide ResNet-50-2 architecture. Over the course of several epochs, the model was able to demonstrate a consistent learning pattern, with both training and testing accuracy steadily fluctuating. The following analysis will focus on the model's learning behavior, performance comparison between training and testing metrics, and possible reasons for the observed results.\n",
        "\n",
        "Training Loss and Accuracy\n",
        "From the results, we observe that the training loss fluctuates around 1.05, gradually decreasing over time. At the start of the training, the loss was 1.058, and as training progressed, it decreased slightly, indicating that the model was learning and adjusting its weights to reduce the discrepancy between predicted and actual labels. By the final epoch, the training loss had reduced further to approximately 1.0048. This shows that the model was able to minimize the error during training, which is a positive sign that the learning process was effective.\n",
        "\n",
        "The training accuracy also follows a similar upward trend. Initially, the training accuracy was around 85.92% and gradually increased over time, with the final epoch reaching 88.5%. This suggests that the model was successfully learning to classify the sea animal species from the training dataset. However, the gradual improvement in accuracy indicates that the model may have reached a point where the performance gains were marginal, potentially pointing to the model nearing its capacity or the possibility of early convergence.\n",
        "\n",
        "Testing Loss and Accuracy\n",
        "While the model showed solid training performance, the testing results revealed slightly different dynamics. The test loss fluctuated between 1.12 and 1.17, with some higher values seen in the earlier epochs. However, in the later stages of training, the test loss stayed relatively consistent around 1.14 to 1.16, showing that the model had not significantly overfit to the training data. This consistency in test loss suggests that the model generalized well, but also indicates that further improvements in accuracy were limited.\n",
        "\n",
        "The test accuracy ranged from 82.17% to 84.01%. While the modelâ€™s test accuracy is competitive, it consistently remained lower than the training accuracy. This difference between the training and testing performance is a typical sign of overfitting or the model not being fully generalized to unseen data. However, the fact that the test accuracy was consistently above 82% indicates that the model was able to generalize well, even though there may have been room for improvement."
      ],
      "metadata": {
        "id": "vTcqOctxHK_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save and Load Model\n",
        "This section handles saving the trained model's state dictionary to disk and later loading it into a new model instance.\n",
        "The model is saved in the \"animal_model\" directory, and a new instance of the model is created by loading the saved state.\n"
      ],
      "metadata": {
        "id": "BBTw-bmQENBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "# Create a directory called \"animal_model\" if it doesn't already exist\n",
        "p = pathlib.Path(\"animal_model\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save the model's state_dict (weights and biases) to a file called 'animal_model_new.pth'\n",
        "torch.save(new_model.state_dict(), \"/content/animal_model/animal_model_new.pth\")\n",
        "\n",
        "# Create a new instance of the model using the same architecture\n",
        "pre_model, transforms_ = created_model()\n",
        "\n",
        "# Load the saved model's state_dict into the new model instance from the saved file\n",
        "pre_model.load_state_dict(torch.load(f=\"/content/animal_model (1).pth\", map_location=torch.device(\"cpu\")))\n",
        "\n",
        "# Print the model to verify its architecture\n",
        "pre_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y59ZEU6Wie1H",
        "outputId": "8f729a00-aa3b-4217-c234-47ae2768eca1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-b192b80c06a2>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pre_model.load_state_dict(torch.load(f=\"/content/animal_model (1).pth\", map_location=torch.device(\"cpu\")))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Sequential(\n",
              "    (0): Dropout(p=0.4, inplace=True)\n",
              "    (1): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=2048, out_features=23, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deployment of out model\n",
        "\n",
        "In this part we will store the files we needed in the folder called deployments and I will upload them into hugging face.\n",
        "\n",
        "The trained model is deployed using the Gradio library to create a user-friendly interface for predictions.\n",
        "\n",
        "The model is loaded into memory and set up to classify images of sea animals.\n",
        "\n",
        "The interface allows users to upload images, get predictions with confidence scores, and measure prediction time."
      ],
      "metadata": {
        "id": "a0pojlYIEdSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_dic=pathlib.Path(\"deployment\").mkdir(parents=True,exist_ok=True)\n",
        "shutil.copy(\"/content/animal_model (1).pth\",\"/content/deployment\")\n",
        "new_example=pathlib.Path(\"deployment/examples\").mkdir(parents=True,exist_ok=True)\n",
        "\n",
        "shutil.copy(\"/content/Our_datata/Fish/101198240_431f4c276b_o.jpg\",\"/content/deployment/examples\")\n",
        "shutil.copy(\"/content/Our_datata/Dolphin/10080987554_27d23b7ca3_o.jpg\",\"/content/deployment/examples\")\n",
        "shutil.copy(\"/content/Our_datata/Corals/10465606544_52913a3632_o.jpg\",\"/content/deployment/examples\")\n",
        "\n",
        "\n",
        "txt_path=pathlib.Path(\"/content/deployment\")/\"class_name.txt\"\n",
        "txt_path\n",
        "\n",
        "with open(txt_path,\"w\") as f:\n",
        "  f.write(\"\\n\".join(Dataset_.classes))"
      ],
      "metadata": {
        "id": "pMZwRjT48sdi"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile deployment/model.py\n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "def created_model(class_n=23):\n",
        "  weights=torchvision.models.Wide_ResNet50_2_Weights.DEFAULT\n",
        "\n",
        "\n",
        "  model=torchvision.models.wide_resnet50_2(weights)\n",
        "  transform_=weights.transforms()\n",
        "  for i in model.parameters():\n",
        "    i.requires_grad=False\n",
        "\n",
        "  model.fc=nn.Sequential(nn.Dropout(p=0.4,inplace=True),nn.Linear(model.fc.in_features,model.fc.in_features),nn.ReLU(),nn.Linear(model.fc.in_features,out_features=class_n))\n",
        "  return model,transform_\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dj8hg0uHSugO",
        "outputId": "2f256bc5-bd83-40b5-eac3-d21d4c338d95"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing deployment/model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile deployment/app.py\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "import gradio as gr  # Gradio library for creating web interfaces\n",
        "import os\n",
        "\n",
        "from model import created_model  # Importing the model creation function\n",
        "from timeit import default_timer as timer  # Timer to measure prediction time\n",
        "\n",
        "# Create model instance and load pre-trained weights\n",
        "pre_model, transforms_ = created_model()\n",
        "pre_model.load_state_dict(torch.load(f=\"/animal_model (1).pth\", map_location=torch.device(\"cpu\")))\n",
        "\n",
        "# Move model to CPU for inference\n",
        "pre_model = pre_model.cpu()\n",
        "\n",
        "# Define the prediction function\n",
        "def prediction_fuc(img):\n",
        "    \"\"\"\n",
        "    Function to make predictions on input images using the trained model.\n",
        "    Args:\n",
        "        img (PIL.Image): The input image for classification.\n",
        "    Returns:\n",
        "        dict: A dictionary with the class names and their respective prediction probabilities.\n",
        "        float: Time taken for prediction.\n",
        "    \"\"\"\n",
        "    start = timer()  # Start timer to measure prediction time\n",
        "\n",
        "    # Preprocess the image (apply transformation and add batch dimension)\n",
        "    img = transforms_(img).unsqueeze(0).to(\"cpu\")\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    pre_model.eval()\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.inference_mode():\n",
        "        logit = pre_model(img).squeeze()  # Get raw logits (model outputs)\n",
        "        pred = torch.softmax(logit, dim=-1)  # Convert logits to probabilities\n",
        "\n",
        "    end = timer()  # End timer\n",
        "\n",
        "    # Return predictions as class names with probabilities and prediction time\n",
        "    return {Dataset_.classes[i]: round(pred[i].item(), 6) for i in range(len(Dataset_.classes))}, round(end - start, 4)\n",
        "\n",
        "# Set up Gradio interface for user input and displaying predictions\n",
        "title = \"Classification of 23 Classes of Sea Animal\"\n",
        "description = \"Wide ResNet50-2 model for classification of sea animals.\"\n",
        "\n",
        "# Example images for testing the model (from the 'examples' folder)\n",
        "example_list = [[\"examples/\" + i] for i in os.listdir(\"examples\")]\n",
        "\n",
        "# Create Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=prediction_fuc,  # The function to call for predictions\n",
        "    inputs=gr.Image(type=\"pil\"),  # Input type is an image (PIL format)\n",
        "    outputs=[gr.Label(num_top_classes=4, label=\"Predictions\"), gr.Number(label=\"Prediction Time (s)\")],  # Outputs: top 4 predictions and time taken\n",
        "    examples=example_list,  # Example images to display for users\n",
        "    title=title,  # Title of the app\n",
        "    description=description  # Description of the app\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmhdHvKgT6lT",
        "outputId": "1c16eac9-5840-4f88-b775-6195f54e6657"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting deployment/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile deployment/requirments.txt\n",
        "\n",
        "torch==2.5.1\n",
        "torchvision==0.20.1\n",
        "gradio==5.8.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipcMhn3jyhLb",
        "outputId": "4c1d3492-9a0f-461d-8678-8896442c2ea8"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing deployment/requirments.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio Deployment for Sea Animal Classification\n",
        "\n",
        "This section deploys the trained model using Gradio to create an interactive web interface for classifying sea animals into 23 classes. Users can upload an image, and the interface will return the top predicted classes along with the prediction time.\n",
        "The model used for classification is Wide ResNet50-2, and it outputs the predicted class probabilities\n",
        "and time taken to perform the classification."
      ],
      "metadata": {
        "id": "S4CsNTDbFmFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "#we write out prediction_fuc for testing\n",
        "\n",
        "test_image=Image.open(\"/content/Our_datata/Corals/10465606544_52913a3632_o.jpg\")\n",
        "def prediction_fuc(img):\n",
        "  \"\"\"\n",
        "    Function to make predictions on input images using the trained model.\n",
        "    Args:\n",
        "        img (PIL.Image): The input image for classification.\n",
        "    Returns:\n",
        "        dict: A dictionary with the class names and their respective prediction probabilities.\n",
        "        float: Time taken for prediction.\n",
        "    \"\"\"\n",
        "\n",
        "  start=timer()\n",
        "  img=transform_(img).unsqueeze(0).to(\"cpu\")\n",
        "  pre_model.eval()\n",
        "  with torch.inference_mode():\n",
        "      logit=pre_model(img).squeeze()\n",
        "      pred=torch.softmax(logit,dim=-1)\n",
        "  end=timer()\n",
        "  return {Dataset_.classes[i]: round(pred[i].item(),6) for i in range(len(Dataset_.classes))},round(end-start,4)\n",
        "\n",
        "\n",
        "\n",
        "#print out one example of prediction_function\n",
        "prediction_fuc(test_image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGzIrabX8Shx",
        "outputId": "44ddf712-11a3-4c55-a4aa-48675e9afa46"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'Clams': 0.003447,\n",
              "  'Corals': 0.771314,\n",
              "  'Crabs': 0.001092,\n",
              "  'Dolphin': 0.000344,\n",
              "  'Eel': 0.004419,\n",
              "  'Fish': 0.159919,\n",
              "  'Jelly Fish': 0.002563,\n",
              "  'Lobster': 0.00181,\n",
              "  'Nudibranchs': 0.000969,\n",
              "  'Octopus': 0.002809,\n",
              "  'Otter': 0.002182,\n",
              "  'Penguin': 0.000675,\n",
              "  'Puffers': 0.004549,\n",
              "  'Sea Rays': 0.00503,\n",
              "  'Sea Urchins': 0.004265,\n",
              "  'Seahorse': 0.001014,\n",
              "  'Seal': 0.000744,\n",
              "  'Sharks': 0.012505,\n",
              "  'Shrimp': 0.000614,\n",
              "  'Squid': 0.004706,\n",
              "  'Starfish': 0.002503,\n",
              "  'Turtle_Tortoise': 0.004895,\n",
              "  'Whale': 0.007633},\n",
              " 0.5081)"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdaU_PewycJ1",
        "outputId": "79f6eaf9-faa6-4941-f560-a40b01061b8d"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.9.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.5.2 (from gradio)\n",
            "  Downloading gradio_client-1.5.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.19-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.8.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.42.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.33.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.9.0-py3-none-any.whl (57.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.5.2-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading python_multipart-0.0.19-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.8.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.33.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.6 ffmpy-0.4.0 gradio-5.9.0 gradio-client-1.5.2 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.19 ruff-0.8.3 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.41.3 tomlkit-0.13.2 uvicorn-0.33.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr  # Gradio for creating web interfaces\n",
        "import os\n",
        "from timeit import default_timer as timer  # Timer to measure prediction time\n",
        "from PIL import Image  # For image processing\n",
        "\n",
        "# Function to make predictions\n",
        "def prediction_fuc(img):\n",
        "    \"\"\"\n",
        "    Function to predict the class of the input image using the trained model.\n",
        "\n",
        "    Args:\n",
        "        img (PIL.Image): The input image for classification.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary of class labels with their respective prediction probabilities.\n",
        "        float: Time taken for making the prediction.\n",
        "    \"\"\"\n",
        "    start = timer()  # Start timer to measure prediction time\n",
        "\n",
        "    # Preprocess the image: apply transformations and add a batch dimension\n",
        "    img = transform_(img).unsqueeze(0).to(\"cpu\")\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    pre_model.eval()\n",
        "\n",
        "    # Perform inference on the image with the model\n",
        "    with torch.inference_mode():\n",
        "        logit = pre_model(img).squeeze()  # Get raw model outputs (logits)\n",
        "        pred = torch.softmax(logit, dim=-1)  # Convert logits to probabilities\n",
        "\n",
        "    end = timer()  # End timer to measure total inference time\n",
        "\n",
        "    # Return the class names with predicted probabilities and the time taken for prediction\n",
        "    return {Dataset_.classes[i]: round(pred[i].item(), 6) for i in range(len(Dataset_.classes))}, round(end - start, 4)\n",
        "\n",
        "# Set up the Gradio demo interface\n",
        "title = \"Classification of 23 Classes of Sea Animal\"\n",
        "description = \"Wide ResNet50-2 model for classification of sea animals.\"\n",
        "\n",
        "# Example images for testing the model (these images are used to demonstrate the functionality of the app)\n",
        "example_list = [[\"/content/deployment/examples/10080987554_27d23b7ca3_o.jpg\"], [\"/content/deployment/examples/10465606544_52913a3632_o.jpg\"]]\n",
        "\n",
        "# Create Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=prediction_fuc,  # The function to call for predictions\n",
        "    inputs=gr.Image(type=\"pil\"),  # The input is an image (PIL format)\n",
        "    outputs=[gr.Label(num_top_classes=4, label=\"Predictions\"), gr.Number(label=\"Prediction Time (s)\")],  # Outputs: top 4 predictions and time taken\n",
        "    examples=example_list,  # Provide example images for testing\n",
        "    title=title,  # Set the title of the interface\n",
        "    description=description  # Set the description of the interface\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface and enable sharing to generate a public URL\n",
        "demo.launch(debug=False, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "-8jbZhuCu3wq",
        "outputId": "83507e57-5386-4a01-d586-1ea7bb5cfe43"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://0e5983dd1739f69c8f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0e5983dd1739f69c8f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    }
  ]
}