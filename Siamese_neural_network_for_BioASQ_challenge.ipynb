{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/garylau1/model_training/blob/main/Siamese_neural_network_for_BioASQ_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9pWsPAt0dCh"
      },
      "source": [
        "Find complex answers to medical questions\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW5l3E6r8Qwm"
      },
      "source": [
        "# Overall Task Review\n",
        "\n",
        "You will work on a task of \"query-focused summarisation\" on medical questions where the goal is, given a medical question and a list of sentences extracted from relevant medical publications, to determine which of these sentences from the list can be used as part of the answer to the question. Assignment 3 is divided into two parts. Part 1 will help you get familar with the data, and Part 2 requires you to implement deep neural networks.\n",
        "\n",
        "We will use data that has been derived from the **BioASQ challenge** (http://www.bioasq.org/), after some data manipulation to make it easier to process for this assignment. The BioASQ challenge organises several \"shared tasks\", including a task on biomedical semantic question answering which we are using here. The data are in the file `bioasq10_labelled.csv`, which is part of the zip file provided. Each row of the file has a question, a sentence text, and a label that indicates whether the sentence text is part of the answer to the question (1) or not (0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_Eji1qY8Qwm"
      },
      "source": [
        "# Data Review\n",
        "\n",
        "The following code uses pandas to store the file `bioasq10_labelled.csv` in a data frame and show the first rows of data. For this code to run, first you need to unzip the file `data.zip`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7F-iu1CC8Qwn"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from keras.layers import LSTM, SimpleRNN\n",
        "\n",
        "import random\n",
        "random.seed(1234)\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing import text as TT\n",
        "import tensorflow.keras.backend as K\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import  Input, Embedding, LSTM, Bidirectional, Dense, Lambda\n",
        "from keras.initializers import Constant\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsEqFlSS8Qwp"
      },
      "source": [
        "The columns of the CSV file are:\n",
        "\n",
        "* `qid`: an ID for a question. Several rows may have the same question ID, as we can see above.\n",
        "* `sentid`: an ID for a sentence.\n",
        "* `question`: The text of the question. In the above example, the first rows all have the same question: \"Is Hirschsprung disease a mendelian or a multifactorial disorder?\"\n",
        "* `sentence text`: The text of the sentence.\n",
        "* `label`: 1 if the sentence is a part of the answer, 0 if the sentence is not part of the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qGQP1eO8Qwq"
      },
      "source": [
        "# Now Let's get started for the Part 2 tasks\n",
        "\n",
        "Use the provided files `training.csv`, `dev_test.csv`, and `test.csv` in the data.zip file for all the tasks below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VTTgRnN0dC4"
      },
      "source": [
        "# Task 1 (5 marks): Simple Siamese NN\n",
        "\n",
        "Implement a simple TensorFlow-Keras neural model that has the following sequence of layers:\n",
        "\n",
        "1. An input layer that will accept the tf.idf of triplet data. The input of Siamese network is a triplet, consisting of anchor (i.e., the question), positive answer, negative answer.\n",
        "2. 3 hidden layers and a relu activation function. You need to determine the size of the hidden layers.\n",
        "3. Implement a class that serves as a distance layer. It returns the squared Euclidean distance between anchor and positive answer, as well as that between anchor and negative answer\n",
        "4. Implement a function that prepares raw data in csv files into triplets. Note that it is important to keep the similar number of positive pairs and negative pairs. For example, if a question has 10 anwsers, then we at most can have 10 positive pairs and it is good to associate this question with 10~20 negative sentences.\n",
        "\n",
        "\n",
        "Train the model with the training data and use the `dev_test` set to determine a good size of the hidden layer.\n",
        "\n",
        "With the model that you have trained, implement a summariser that returns the $n$ sentences with highest predicted score. Use the following function signature:\n",
        "\n",
        "```{python}\n",
        "def nn_summariser(csvfile, questionids, n=1):\n",
        "   \"\"\"Return the IDs of the n sentences that have the highest predicted score.\n",
        "      The input questionids is a list of question ids.\n",
        "      The output is a list of lists of sentence ids\n",
        "   \"\"\"\n",
        "\n",
        "```\n",
        "\n",
        "Report the final results using the test set. Remember: use the test set to report the final results of the best system only.\n",
        "\n",
        "The breakdown of marks is as follows:\n",
        "\n",
        "* **1 mark** if the NN model has the correct layers, the correct activation functions, and the correct loss function.\n",
        "* **1 mark** if the code passes input to the model correctly.\n",
        "* **1 mark** if the code returns the IDs of the $n$ sentences that have the highest prediction score in the given question.\n",
        "* **1 mark** if the notebook reports the F1 scores of the test sets and comments on the results.\n",
        "* **1 mark** for good coding and documentation in this task. In particular, the code and results must include evidence that shows your choice of best size of the hidden layer. The explanations must be clear and concise. To make this task less time-consuming, use $n=1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DXhWqelAYva"
      },
      "source": [
        "#### Data preprocessing:\n",
        "\n",
        "- In this part we implement a function that prepares raw data in csv files into triplets by creating a function random_triplet_generator.We keep the exact same number of positive pairs and negative pairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGaVOR-Q7U7x"
      },
      "outputs": [],
      "source": [
        "def random_triplet_generator(x):\n",
        "  # we create a function for preparing triplets with set of anchor,positive and negative texts.\n",
        "  data = pd.read_csv(x)\n",
        "  positive_data=data[data[\"label\"]==1]                   #this part is mainly for anchor and the positive part.\n",
        "  a_input=positive_data[\"question\"].to_list()\n",
        "  p_input=positive_data[\"sentence text\"].to_list()\n",
        "\n",
        "  a_input=np.array(a_input)\n",
        "  p_input=np.array(p_input)\n",
        "\n",
        "  negative_data=data[data[\"label\"]==0]                 #this part is mainly for negative part.\n",
        "  n_input= negative_data[\"sentence text\"].to_list()\n",
        "  n_input=np.array(n_input)\n",
        "  import random\n",
        "\n",
        "  random.seed(10)\n",
        "  ind = [i for i in range(len(a_input))]                # we create two index lists ind and ind_2 to randomly shuffle the data for preventing overfitting\n",
        "  random.shuffle(ind)\n",
        "\n",
        "  random.seed(1212)\n",
        "  ind_2 = [i for i in range(len(n_input))]\n",
        "  random.shuffle(ind_2)\n",
        "\n",
        "\n",
        "  a_input=a_input[ind].tolist()                       #  These are the randomly triplet set we created\n",
        "  n_input=n_input[ind_2].tolist()\n",
        "  p_input=p_input[ind].tolist()\n",
        "\n",
        "  return (a_input,p_input,n_input[0:len(a_input)])   #we need three pairs of data the same length.#\n",
        "\n",
        "# this is the triplet data set with the same length of both positive and  negative texts\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBbfpymFWqiE"
      },
      "outputs": [],
      "source": [
        "a_input,p_input,n_input=random_triplet_generator('training.csv')                   #we created a set of triplet in train,dev_test and test sets\n",
        "a_input_test,p_input_test,n_input_test=random_triplet_generator('dev_test.csv')\n",
        "a_input_test2,p_input_test2,n_input_test2=random_triplet_generator('test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAdnJGihAwGR"
      },
      "source": [
        "### tf.idf vector:\n",
        "- We create the tf.idf of triplet data for our train,set and dev_set :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TacCB2qa7VjQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "MAX_LEN=7000               #we only use the first MAX_LEN numbers of  higheest tfidf score to train our models\n",
        "#EPOCH=9\n",
        "batch_size = 40           #set our batch size\n",
        "\n",
        "from tkinter import Text\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer         #\n",
        "\n",
        "\n",
        "\n",
        "tfidf=TfidfVectorizer(stop_words=\"english\",max_features=MAX_LEN)      # We convert into tfidf for each questions and answers\n",
        "#firstly we create the TfidfVectorizer object with max_features\n",
        "\n",
        "tfidf.fit(a_input+p_input+n_input)           #we fit the tfidf\n",
        "\n",
        "\n",
        "a_input=tfidf.transform(a_input).toarray()                  #convert our questions and answers triplet into tfidf vectors\n",
        "n_input=tfidf.transform(n_input).toarray()\n",
        "p_input=tfidf.transform(p_input).toarray()\n",
        "\n",
        "a_input_test=tfidf.transform(a_input_test).toarray()\n",
        "n_input_test=tfidf.transform(n_input_test).toarray()\n",
        "p_input_test=tfidf.transform(p_input_test).toarray()\n",
        "\n",
        "a_input_test2=tfidf.transform(a_input_test2).toarray()\n",
        "n_input_test2=tfidf.transform(n_input_test2).toarray()\n",
        "p_input_test2=tfidf.transform(p_input_test2).toarray()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF-HnqmvBGM5"
      },
      "source": [
        "#### We create our model with the requirement:\n",
        "\n",
        "- The input of Siamese network is a triplet, consisting of anchor (i.e., the question), positive answer, negative answer.\n",
        "- 3 hidden layers and a relu activation function.\n",
        "- Implement a class that serves as a distance layer. It returns the squared Euclidean distance between anchor and positive answer, as well as that between anchor and negative answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uY6sDbUn0dC6"
      },
      "outputs": [],
      "source": [
        "first_sent_in_a = Input(shape=(MAX_LEN,))   #we create the input with the max_len input shape to fit our model\n",
        "second_sent_in_p = Input(shape=(MAX_LEN,))\n",
        "third_sent_in_n = Input(shape=(MAX_LEN,))\n",
        "\n",
        "from keras.models import Model, Sequential\n",
        "import tensorflow as tf\n",
        "from keras.layers import  Input, Embedding, LSTM, Bidirectional, Dense, Lambda\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "def siamese_network_generator(x,y,z):  #we create a function for building the siamese_network with 3 inputs of the hidden layers\n",
        "    first_sent_in_a = Input(shape=(MAX_LEN,))   #we create the input with the max_len input shape to fit our model\n",
        "    second_sent_in_p = Input(shape=(MAX_LEN,))\n",
        "    third_sent_in_n = Input(shape=(MAX_LEN,))\n",
        "\n",
        "\n",
        "    model = Sequential([\n",
        "      Dense(x, activation='relu'),\n",
        "        Dense(y, activation='relu'),     #we have three hidden layers into the model\n",
        "         Dense(z, activation='relu')\n",
        "    ])\n",
        "\n",
        "    encoded_1 = model(first_sent_in_a)         #these are the output where the datas passed through 3 hidden layers\n",
        "    encoded_2 = model(second_sent_in_p)\n",
        "    encoded_3 = model(third_sent_in_n)\n",
        "\n",
        "\n",
        "    class distance_layer(Layer):   #create a distance layer as required\n",
        "\n",
        "\n",
        "        def __init__(self, **kwargs):\n",
        "            super().__init__(**kwargs)   #we need inheritance from the superclass layer.\n",
        "\n",
        "        def call(self, anchor, positive, negative):\n",
        "            ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)    #we caluclate the distance between the lists of positive(negative) and anchor vector\n",
        "            an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
        "            return (ap_distance, an_distance)  #Returns the squared Euclidean distance between anchor and positive answer, as well as that between anchor and negative answer\n",
        "\n",
        "\n",
        "    loss_layer = distance_layer()(encoded_1, encoded_2, encoded_3)  #this is the output\n",
        "\n",
        "    siamese_network = Model([first_sent_in_a, second_sent_in_p, third_sent_in_n], loss_layer)  #we define our siamese_network here\n",
        "\n",
        "    return siamese_network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltEBppsNBczr",
        "outputId": "8770da38-4d5f-4b78-e1e9-84eac60ff686"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, 7000)]       0           []                               \n",
            "                                                                                                  \n",
            " input_5 (InputLayer)           [(None, 7000)]       0           []                               \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)           [(None, 7000)]       0           []                               \n",
            "                                                                                                  \n",
            " sequential (Sequential)        (None, 32)           373837      ['input_4[0][0]',                \n",
            "                                                                  'input_5[0][0]',                \n",
            "                                                                  'input_6[0][0]']                \n",
            "                                                                                                  \n",
            " distance_layer (distance_layer  ((None,),           0           ['sequential[0][0]',             \n",
            " )                               (None,))                         'sequential[1][0]',             \n",
            "                                                                  'sequential[2][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 373,837\n",
            "Trainable params: 373,837\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "siamese_network=siamese_network_generator(53,32,32)\n",
        "\n",
        "siamese_network.summary()  #this is an example of how we create the layers of this model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWEel4GoawIV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "from keras import applications\n",
        "from keras import layers\n",
        "from keras import losses\n",
        "\n",
        "from keras import optimizers\n",
        "from keras import metrics\n",
        "from keras import Model\n",
        "from keras.applications import resnet\n",
        "\n",
        "\n",
        "class SiameseModel(Model):\n",
        "# next we define our custom model class as our task is not traditional classification task\n",
        "#there are lot of thing needed to be changed\n",
        "  def __init__(self, siamese_network, margin=0.00000000000000000005):   #we choose such value;if the margin value is too large,the lost will not decrease after long period.\n",
        "    super().__init__()\n",
        "    self.siamese_network = siamese_network                                              # We create a build and create those object\n",
        "    self.loss_tracker = metrics.Mean(name=\"loss\")                  #We define our own metrics with the loss function \"triplet_lost\"  we define below\n",
        "    self.margin = margin                                          #this is the margin value defined for our triplet loss\n",
        "  def call(self, inputs):\n",
        "    return self.siamese_network(inputs)\n",
        "  @tf.function\n",
        "  def train_step(self, data):                         #we define our custom training step with trainable_weights\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = self.triplet_lost(data)\n",
        "        trainable_vars = self.siamese_network.trainable_weights\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        self.optimizer.apply_gradients(                                     #we use gradient descent to keep track of each steps\n",
        "            zip(gradients, trainable_vars)\n",
        "        )\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}                           # return the loss for each steps\n",
        "  @tf.function\n",
        "  def test_step(self, data):\n",
        "        loss = self.triplet_lost(data)\n",
        "                                                                                    #we do the same thing for test step\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss_3\": self.loss_tracker.result()}                             #the loss function loss_3 is for triplet validation dataset or test set (depended on situation)\n",
        "  @tf.function\n",
        "  def triplet_lost(self, data):\n",
        "\n",
        "\n",
        "        ap_distance, an_distance = self.siamese_network(data)                  # we defined the triplet loss fucntion inside our custom fit\n",
        "\n",
        "        loss=(tf.maximum(ap_distance - an_distance + self.margin, 0.0))\n",
        "        loss=tf.reduce_sum(loss,0)\n",
        "\n",
        "        return loss\n",
        "  @property\n",
        "  def metrics(self):\n",
        "\n",
        "    return [self.loss_tracker]                              #we need our loss tracker\n",
        "\n",
        "          #print out the detail of model we created\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSjRTNjTBUep"
      },
      "source": [
        "#### Finding the best hyperparamters\n",
        "\n",
        "- In this part we search for different input of parameters and find the best paramters for least validation loss\n",
        "\n",
        "- to simplify the process,we choose different hidden sizes to test the performance but we use the same number of hidden sizes in all 3 hidden layers and we set a fix learning rate with a small margin value(because the validation lost didnt go down after lone period)\n",
        "\n",
        "- We set very small margins due to higher one gives no result.we just make it simple by looking at whether ap_distance is smaller than an_distance only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTYp74CbnJi2"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import callbacks\n",
        "# we set a  function evluate the validation loss by puting different hyperparamters\n",
        "def validation_evaluation(num_1,num_2,num_3,learning_rate,alpha):   #this function gives a input of parameters for each layers,learning rate and margin values\n",
        "  siamese_model = SiameseModel(siamese_network_generator(num_1,num_2,num_3),alpha)\n",
        "  siamese_model.compile(optimizer=optimizers.Adam(learning_rate))\n",
        "  callback = callbacks.EarlyStopping(monitor='loss',patience=2)\n",
        "  siamese_model.fit([a_input, p_input,n_input],epochs=10, batch_size = 80,validation_data=[a_input_test[0:1000], p_input_test[0:1000],n_input_test[0:1000]],callbacks=[callback])\n",
        "  return (siamese_model.evaluate([a_input_test,p_input_test,n_input_test],batch_size = 80),[num_1,num_2,num_3,learning_rate,alpha]) #the output is a the evaluation score\n",
        "# we use 1000 datas for our validation set because it involved approxmiately 1/10 of the training data~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeQ4QdK8wj7_",
        "outputId": "a0e414fc-c2fb-4786-de02-dd58d24cf18a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "138/143 [===========================>..] - ETA: 0s - loss: 1.3482e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 7ms/step - loss: 1.3027e-04 - val_loss_3: 2.1970e-08\n",
            "Epoch 2/10\n",
            "139/143 [============================>.] - ETA: 0s - loss: 3.8916e-06WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 4.2398e-06 - val_loss_3: 4.8059e-07\n",
            "Epoch 3/10\n",
            "132/143 [==========================>...] - ETA: 0s - loss: 6.3172e-06WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 5.9712e-06 - val_loss_3: 6.5951e-07\n",
            "Epoch 4/10\n",
            "134/143 [===========================>..] - ETA: 0s - loss: 2.6643e-07WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 2.5449e-07 - val_loss_3: 9.3379e-09\n",
            "Epoch 5/10\n",
            "136/143 [===========================>..] - ETA: 0s - loss: 8.0659e-09WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 1.2284e-08 - val_loss_3: 4.7572e-09\n",
            "Epoch 6/10\n",
            "135/143 [===========================>..] - ETA: 0s - loss: 7.2205e-09WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 6.9285e-09 - val_loss_3: 2.9484e-09\n",
            "Epoch 7/10\n",
            "134/143 [===========================>..] - ETA: 0s - loss: 4.6285e-09WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 4.5327e-09 - val_loss_3: 2.3696e-09\n",
            "Epoch 8/10\n",
            "138/143 [===========================>..] - ETA: 0s - loss: 3.4639e-09WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 3.3799e-09 - val_loss_3: 1.7987e-09\n",
            "Epoch 9/10\n",
            "139/143 [============================>.] - ETA: 0s - loss: 2.6262e-09WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 2.5632e-09 - val_loss_3: 1.4073e-09\n",
            "Epoch 10/10\n",
            "139/143 [============================>.] - ETA: 0s - loss: 2.1022e-09WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 2.0540e-09 - val_loss_3: 1.2230e-09\n",
            "49/49 [==============================] - 0s 2ms/step - loss_3: 1.6160e-09\n",
            "Epoch 1/10\n",
            "141/143 [============================>.] - ETA: 0s - loss: 4.7537e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 6ms/step - loss: 4.6882e-04 - val_loss_3: 9.9682e-06\n",
            "Epoch 2/10\n",
            "136/143 [===========================>..] - ETA: 0s - loss: 1.3191e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 1.2768e-05 - val_loss_3: 3.7406e-06\n",
            "Epoch 3/10\n",
            "140/143 [============================>.] - ETA: 0s - loss: 2.1778e-06WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 2.2259e-06 - val_loss_3: 1.9833e-06\n",
            "Epoch 4/10\n",
            "141/143 [============================>.] - ETA: 0s - loss: 1.6151e-06WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 1.5977e-06 - val_loss_3: 2.4158e-06\n",
            "Epoch 5/10\n",
            "141/143 [============================>.] - ETA: 0s - loss: 7.4147e-07WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 7.3231e-07 - val_loss_3: 1.5846e-06\n",
            "Epoch 6/10\n",
            "140/143 [============================>.] - ETA: 0s - loss: 1.7807e-06WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 1.8624e-06 - val_loss_3: 7.6206e-06\n",
            "Epoch 7/10\n",
            "139/143 [============================>.] - ETA: 0s - loss: 7.9409e-07WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 7.7788e-07 - val_loss_3: 1.1932e-06\n",
            "49/49 [==============================] - 0s 2ms/step - loss_3: 1.7019e-06\n",
            "Epoch 1/10\n",
            "138/143 [===========================>..] - ETA: 0s - loss: 6.3779e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "143/143 [==============================] - 1s 7ms/step - loss: 6.1584e-04 - val_loss_3: 9.8262e-06\n",
            "Epoch 2/10\n",
            "136/143 [===========================>..] - ETA: 0s - loss: 8.8863e-06WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 8.5710e-06 - val_loss_3: 3.3007e-06\n",
            "Epoch 3/10\n",
            "140/143 [============================>.] - ETA: 0s - loss: 5.3278e-06WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 5.2835e-06 - val_loss_3: 2.9161e-06\n",
            "Epoch 4/10\n",
            "142/143 [============================>.] - ETA: 0s - loss: 2.6047e-06WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 2.5876e-06 - val_loss_3: 2.4903e-06\n",
            "Epoch 5/10\n",
            "132/143 [==========================>...] - ETA: 0s - loss: 8.5712e-07WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 8.2848e-07 - val_loss_3: 2.3758e-06\n",
            "Epoch 6/10\n",
            "133/143 [==========================>...] - ETA: 0s - loss: 1.2387e-07WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 1.2217e-07 - val_loss_3: 1.6091e-06\n",
            "Epoch 7/10\n",
            "132/143 [==========================>...] - ETA: 0s - loss: 6.0185e-08WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 5.9358e-08 - val_loss_3: 1.3786e-06\n",
            "Epoch 8/10\n",
            "140/143 [============================>.] - ETA: 0s - loss: 2.8865e-08WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 2.9370e-08 - val_loss_3: 1.2094e-06\n",
            "Epoch 9/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 2.5029e-08WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 2.5029e-08 - val_loss_3: 1.1888e-06\n",
            "Epoch 10/10\n",
            "133/143 [==========================>...] - ETA: 0s - loss: 1.1771e-08WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 1.2442e-08 - val_loss_3: 1.1277e-06\n",
            "49/49 [==============================] - 0s 2ms/step - loss_3: 8.5016e-06\n",
            "Epoch 1/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 7.9119e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 8ms/step - loss: 7.9119e-04 - val_loss_3: 1.0197e-05\n",
            "Epoch 2/10\n",
            "137/143 [===========================>..] - ETA: 0s - loss: 1.3606e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 6ms/step - loss: 1.3381e-05 - val_loss_3: 3.0342e-06\n",
            "Epoch 3/10\n",
            "135/143 [===========================>..] - ETA: 0s - loss: 8.3776e-06WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 6ms/step - loss: 8.0495e-06 - val_loss_3: 4.8583e-06\n",
            "Epoch 4/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 2.2148e-06WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 6ms/step - loss: 2.2148e-06 - val_loss_3: 2.1712e-06\n",
            "Epoch 5/10\n",
            "135/143 [===========================>..] - ETA: 0s - loss: 4.8891e-07WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 6ms/step - loss: 4.8400e-07 - val_loss_3: 3.3129e-06\n",
            "Epoch 6/10\n",
            "136/143 [===========================>..] - ETA: 0s - loss: 4.8299e-07WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 6ms/step - loss: 4.6464e-07 - val_loss_3: 2.0442e-06\n",
            "Epoch 7/10\n",
            "136/143 [===========================>..] - ETA: 0s - loss: 2.4287e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 6ms/step - loss: 2.3334e-05 - val_loss_3: 9.7676e-06\n",
            "Epoch 8/10\n",
            "135/143 [===========================>..] - ETA: 0s - loss: 9.7224e-06WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 6ms/step - loss: 9.9842e-06 - val_loss_3: 4.5429e-06\n",
            "49/49 [==============================] - 0s 3ms/step - loss_3: 1.1090e-05\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "136/143 [===========================>..] - ETA: 0s - loss: 0.0025WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 10ms/step - loss: 0.0023 - val_loss_3: 1.0657e-04\n",
            "Epoch 2/10\n",
            "135/143 [===========================>..] - ETA: 0s - loss: 4.9028e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 7ms/step - loss: 4.7482e-05 - val_loss_3: 3.2508e-05\n",
            "Epoch 3/10\n",
            "138/143 [===========================>..] - ETA: 0s - loss: 1.1403e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 8ms/step - loss: 1.1120e-05 - val_loss_3: 2.7458e-05\n",
            "Epoch 4/10\n",
            "136/143 [===========================>..] - ETA: 0s - loss: 4.7165e-06WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 8ms/step - loss: 4.5574e-06 - val_loss_3: 2.6226e-05\n",
            "Epoch 5/10\n",
            "138/143 [===========================>..] - ETA: 0s - loss: 3.1159e-06WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 7ms/step - loss: 3.0284e-06 - val_loss_3: 2.5769e-05\n",
            "Epoch 6/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 4.0897e-06WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 7ms/step - loss: 4.0897e-06 - val_loss_3: 3.4624e-05\n",
            "Epoch 7/10\n",
            "138/143 [===========================>..] - ETA: 0s - loss: 2.7507e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 7ms/step - loss: 2.6885e-05 - val_loss_3: 6.9561e-05\n",
            "49/49 [==============================] - 0s 4ms/step - loss_3: 8.3092e-05\n",
            "Epoch 1/10\n",
            "136/143 [===========================>..] - ETA: 0s - loss: 0.0018WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 9ms/step - loss: 0.0017 - val_loss_3: 3.8126e-05\n",
            "Epoch 2/10\n",
            "136/143 [===========================>..] - ETA: 0s - loss: 1.9935e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 7ms/step - loss: 1.9276e-05 - val_loss_3: 1.2067e-05\n",
            "Epoch 3/10\n",
            "137/143 [===========================>..] - ETA: 0s - loss: 4.2594e-06WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 7ms/step - loss: 4.1415e-06 - val_loss_3: 9.5995e-06\n",
            "Epoch 4/10\n",
            "137/143 [===========================>..] - ETA: 0s - loss: 1.0499e-06WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 7ms/step - loss: 1.0376e-06 - val_loss_3: 8.8412e-06\n",
            "Epoch 5/10\n",
            "137/143 [===========================>..] - ETA: 0s - loss: 2.6291e-07WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 7ms/step - loss: 2.7495e-07 - val_loss_3: 8.2059e-06\n",
            "Epoch 6/10\n",
            "136/143 [===========================>..] - ETA: 0s - loss: 5.9182e-06WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 8ms/step - loss: 5.6409e-06 - val_loss_3: 7.1041e-05\n",
            "Epoch 7/10\n",
            "139/143 [============================>.] - ETA: 0s - loss: 2.3444e-07WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 7ms/step - loss: 1.0627e-06 - val_loss_3: 1.5401e-04\n",
            "49/49 [==============================] - 0s 3ms/step - loss_3: 9.3071e-05\n",
            "Epoch 1/10\n",
            "137/143 [===========================>..] - ETA: 0s - loss: 0.0046WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 13ms/step - loss: 0.0044 - val_loss_3: 2.0805e-04\n",
            "Epoch 2/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 7.6557e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 9ms/step - loss: 7.6557e-05 - val_loss_3: 8.7032e-05\n",
            "Epoch 3/10\n",
            "141/143 [============================>.] - ETA: 0s - loss: 1.7284e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 9ms/step - loss: 1.7246e-05 - val_loss_3: 8.5756e-05\n",
            "Epoch 4/10\n",
            "139/143 [============================>.] - ETA: 0s - loss: 1.2883e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "143/143 [==============================] - 1s 9ms/step - loss: 1.2761e-05 - val_loss_3: 1.0435e-04\n",
            "Epoch 5/10\n",
            "139/143 [============================>.] - ETA: 0s - loss: 9.1703e-06WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 8ms/step - loss: 9.5911e-06 - val_loss_3: 1.4970e-04\n",
            "Epoch 6/10\n",
            "141/143 [============================>.] - ETA: 0s - loss: 2.8014e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 9ms/step - loss: 2.7669e-05 - val_loss_3: 1.2963e-04\n",
            "Epoch 7/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 5.2238e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 9ms/step - loss: 5.2238e-05 - val_loss_3: 2.3326e-04\n",
            "49/49 [==============================] - 0s 4ms/step - loss_3: 2.4980e-04\n",
            "Epoch 1/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 0.0036WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 13ms/step - loss: 0.0036 - val_loss_3: 2.7351e-04\n",
            "Epoch 2/10\n",
            "139/143 [============================>.] - ETA: 0s - loss: 9.5741e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 10ms/step - loss: 9.4317e-05 - val_loss_3: 1.6399e-04\n",
            "Epoch 3/10\n",
            "142/143 [============================>.] - ETA: 0s - loss: 2.5043e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 9ms/step - loss: 2.5769e-05 - val_loss_3: 2.0509e-04\n",
            "Epoch 4/10\n",
            "138/143 [===========================>..] - ETA: 0s - loss: 2.7581e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 9ms/step - loss: 2.8032e-05 - val_loss_3: 3.0026e-04\n",
            "Epoch 5/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 5.9480e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 9ms/step - loss: 5.9480e-05 - val_loss_3: 4.3941e-04\n",
            "49/49 [==============================] - 0s 4ms/step - loss_3: 4.9607e-04\n",
            "Epoch 1/10\n",
            "141/143 [============================>.] - ETA: 0s - loss: 0.0051WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 4s 18ms/step - loss: 0.0050 - val_loss_3: 4.7399e-04\n",
            "Epoch 2/10\n",
            "142/143 [============================>.] - ETA: 0s - loss: 1.3919e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 3s 21ms/step - loss: 1.3901e-04 - val_loss_3: 2.9005e-04\n",
            "Epoch 3/10\n",
            "142/143 [============================>.] - ETA: 0s - loss: 3.3841e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 3s 20ms/step - loss: 3.3776e-05 - val_loss_3: 2.4887e-04\n",
            "Epoch 4/10\n",
            "141/143 [============================>.] - ETA: 0s - loss: 3.9883e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 3s 20ms/step - loss: 3.9698e-05 - val_loss_3: 3.0451e-04\n",
            "Epoch 5/10\n",
            "142/143 [============================>.] - ETA: 0s - loss: 3.9794e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 3s 19ms/step - loss: 3.9834e-05 - val_loss_3: 3.8282e-04\n",
            "49/49 [==============================] - 1s 12ms/step - loss_3: 4.0270e-04\n",
            "Epoch 1/10\n",
            "142/143 [============================>.] - ETA: 0s - loss: 0.0066WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 4s 24ms/step - loss: 0.0066 - val_loss_3: 8.2332e-04\n",
            "Epoch 2/10\n",
            "141/143 [============================>.] - ETA: 0s - loss: 2.0647e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 3s 22ms/step - loss: 2.0473e-04 - val_loss_3: 4.0588e-04\n",
            "Epoch 3/10\n",
            "142/143 [============================>.] - ETA: 0s - loss: 6.6509e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 3s 22ms/step - loss: 6.6062e-05 - val_loss_3: 4.7694e-04\n",
            "Epoch 4/10\n",
            "142/143 [============================>.] - ETA: 0s - loss: 8.8114e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 3s 21ms/step - loss: 8.7639e-05 - val_loss_3: 7.6059e-04\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10\n",
            "142/143 [============================>.] - ETA: 0s - loss: 1.8224e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 3s 22ms/step - loss: 1.8152e-04 - val_loss_3: 0.0013\n",
            "49/49 [==============================] - 1s 12ms/step - loss_3: 0.0014\n",
            "Epoch 1/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 0.0053WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 3s 21ms/step - loss: 0.0053 - val_loss_3: 3.2440e-04\n",
            "Epoch 2/10\n",
            "141/143 [============================>.] - ETA: 0s - loss: 8.6895e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 14ms/step - loss: 8.6013e-05 - val_loss_3: 1.8436e-04\n",
            "Epoch 3/10\n",
            "140/143 [============================>.] - ETA: 0s - loss: 2.9137e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 15ms/step - loss: 2.9063e-05 - val_loss_3: 2.1295e-04\n",
            "Epoch 4/10\n",
            "141/143 [============================>.] - ETA: 0s - loss: 1.2129e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 15ms/step - loss: 1.2717e-05 - val_loss_3: 1.7761e-04\n",
            "Epoch 5/10\n",
            "141/143 [============================>.] - ETA: 0s - loss: 2.5135e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 14ms/step - loss: 2.4887e-05 - val_loss_3: 2.4393e-04\n",
            "Epoch 6/10\n",
            "141/143 [============================>.] - ETA: 0s - loss: 2.3835e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 14ms/step - loss: 2.4042e-05 - val_loss_3: 3.6224e-04\n",
            "49/49 [==============================] - 0s 7ms/step - loss_3: 3.5721e-04\n",
            "Epoch 1/10\n",
            "140/143 [============================>.] - ETA: 0s - loss: 0.0094WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 3s 16ms/step - loss: 0.0092 - val_loss_3: 0.0013\n",
            "Epoch 2/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 3.3791e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 14ms/step - loss: 3.3791e-04 - val_loss_3: 8.0795e-04\n",
            "Epoch 3/10\n",
            "141/143 [============================>.] - ETA: 0s - loss: 1.4068e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 14ms/step - loss: 1.3937e-04 - val_loss_3: 6.7904e-04\n",
            "Epoch 4/10\n",
            "139/143 [============================>.] - ETA: 0s - loss: 1.0083e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 14ms/step - loss: 1.0085e-04 - val_loss_3: 9.7961e-04\n",
            "Epoch 5/10\n",
            "142/143 [============================>.] - ETA: 0s - loss: 9.1841e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 13ms/step - loss: 9.1672e-05 - val_loss_3: 0.0011\n",
            "Epoch 6/10\n",
            "140/143 [============================>.] - ETA: 0s - loss: 2.9313e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 14ms/step - loss: 3.0023e-04 - val_loss_3: 0.0018\n",
            "Epoch 7/10\n",
            "142/143 [============================>.] - ETA: 0s - loss: 9.4678e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 13ms/step - loss: 9.4764e-04 - val_loss_3: 0.0020\n",
            "49/49 [==============================] - 0s 7ms/step - loss_3: 0.0020\n",
            "Epoch 1/10\n",
            "142/143 [============================>.] - ETA: 0s - loss: 0.0106WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 3s 16ms/step - loss: 0.0105 - val_loss_3: 0.0012\n",
            "Epoch 2/10\n",
            "141/143 [============================>.] - ETA: 0s - loss: 3.1397e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 13ms/step - loss: 3.1209e-04 - val_loss_3: 8.2642e-04\n",
            "Epoch 3/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 1.3338e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 13ms/step - loss: 1.3338e-04 - val_loss_3: 7.4139e-04\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "140/143 [============================>.] - ETA: 0s - loss: 1.2318e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 14ms/step - loss: 1.2201e-04 - val_loss_3: 0.0011\n",
            "Epoch 5/10\n",
            "140/143 [============================>.] - ETA: 0s - loss: 2.7826e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 14ms/step - loss: 2.8762e-04 - val_loss_3: 0.0018\n",
            "Epoch 6/10\n",
            "142/143 [============================>.] - ETA: 0s - loss: 0.0018WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 13ms/step - loss: 0.0018 - val_loss_3: 0.0017\n",
            "49/49 [==============================] - 0s 5ms/step - loss_3: 0.0019\n",
            "Epoch 1/10\n",
            "141/143 [============================>.] - ETA: 0s - loss: 0.0120WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 3s 15ms/step - loss: 0.0119 - val_loss_3: 0.0016\n",
            "Epoch 2/10\n",
            "140/143 [============================>.] - ETA: 0s - loss: 4.3300e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 14ms/step - loss: 4.2978e-04 - val_loss_3: 7.4762e-04\n",
            "Epoch 3/10\n",
            "140/143 [============================>.] - ETA: 0s - loss: 1.2695e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 17ms/step - loss: 1.2689e-04 - val_loss_3: 7.3903e-04\n",
            "Epoch 4/10\n",
            "141/143 [============================>.] - ETA: 0s - loss: 8.6046e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 16ms/step - loss: 8.8000e-05 - val_loss_3: 0.0011\n",
            "Epoch 5/10\n",
            "141/143 [============================>.] - ETA: 0s - loss: 1.6314e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 15ms/step - loss: 1.6671e-04 - val_loss_3: 0.0021\n",
            "Epoch 6/10\n",
            "141/143 [============================>.] - ETA: 0s - loss: 0.0022WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 15ms/step - loss: 0.0023 - val_loss_3: 0.0064\n",
            "49/49 [==============================] - 0s 5ms/step - loss_3: 0.0075\n",
            "Epoch 1/10\n",
            "140/143 [============================>.] - ETA: 0s - loss: 0.0116WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 3s 18ms/step - loss: 0.0114 - val_loss_3: 0.0015\n",
            "Epoch 2/10\n",
            "141/143 [============================>.] - ETA: 0s - loss: 3.7821e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 15ms/step - loss: 3.7906e-04 - val_loss_3: 8.9985e-04\n",
            "Epoch 3/10\n",
            "142/143 [============================>.] - ETA: 0s - loss: 1.5404e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 14ms/step - loss: 1.5323e-04 - val_loss_3: 8.7491e-04\n",
            "Epoch 4/10\n",
            "141/143 [============================>.] - ETA: 0s - loss: 2.1410e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 14ms/step - loss: 2.1396e-04 - val_loss_3: 0.0019\n",
            "Epoch 5/10\n",
            "141/143 [============================>.] - ETA: 0s - loss: 0.0012WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 2s 14ms/step - loss: 0.0013 - val_loss_3: 0.0067\n",
            "49/49 [==============================] - 0s 5ms/step - loss_3: 0.0064\n"
          ]
        }
      ],
      "source": [
        "# we have three trial with different hyperparameters\n",
        "score_list=[]                         # We want to see determine best performance of each model\n",
        "for i in range(16,256,16):    #to be simple: I only use the same hyperparamters for each layers #we have done for 10 different numbers\n",
        "  score_list.append(validation_evaluation(i,i,i,0.005,0.000000000005))\n",
        "\n",
        "#set very small margins due to higher one gives no result #we just make it simple by looking at\n",
        "  #if ap_distance is smaller than an_distance only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mfo_40tz_Wjm",
        "outputId": "e0347279-0f6a-45d2-b531-b579929e305b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[16, 16, 16, 0.005, 5e-12]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "minimum_lost=100000\n",
        "\n",
        "for i , j in (score_list):   # we search for the best minimum losts with the best_praramters\n",
        "  if i<minimum_lost:\n",
        "    best_praramters=j\n",
        "    minimum_lost=i\n",
        "\n",
        "best_praramters     #we select the best hyperparameters from the value with minimum lost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "921Zsg-OB9Mw"
      },
      "source": [
        "#### we retrain the model using best hyperparameters and make prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdcS7KVHe08G",
        "outputId": "efabf450-0c05-4b4b-e12c-64b9567cbd72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "141/143 [============================>.] - ETA: 0s - loss: 0.1975WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 6ms/step - loss: 0.1963 - val_loss_3: 0.2243\n",
            "Epoch 2/10\n",
            "138/143 [===========================>..] - ETA: 0s - loss: 0.0398WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0398 - val_loss_3: 0.2653\n",
            "Epoch 3/10\n",
            "138/143 [===========================>..] - ETA: 0s - loss: 0.0173WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0172 - val_loss_3: 0.2864\n",
            "Epoch 4/10\n",
            "139/143 [============================>.] - ETA: 0s - loss: 0.0153WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0154 - val_loss_3: 0.3192\n",
            "Epoch 5/10\n",
            "136/143 [===========================>..] - ETA: 0s - loss: 0.0147WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0151 - val_loss_3: 0.3441\n",
            "Epoch 6/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 0.0113WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0113 - val_loss_3: 0.3566\n",
            "Epoch 7/10\n",
            "128/143 [=========================>....] - ETA: 0s - loss: 0.0124WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0134 - val_loss_3: 0.3588\n",
            "Epoch 8/10\n",
            "131/143 [==========================>...] - ETA: 0s - loss: 0.0137WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0138 - val_loss_3: 0.3711\n",
            "286/286 [==============================] - 1s 2ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(array([0.01161807, 0.02046644, 0.00955711, ..., 0.00878781, 0.02620158,\n",
              "        0.01148407], dtype=float32),\n",
              " array([0.0485378 , 0.07154507, 0.06493345, ..., 0.10847147, 0.1692521 ,\n",
              "        0.0499534 ], dtype=float32))"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# retrain the model using best hyperparameters and make prediction\n",
        "\n",
        "siamese_model = SiameseModel(siamese_network_generator(best_praramters[0],best_praramters[1],best_praramters[2]),0.005)\n",
        "siamese_model.compile(optimizer=optimizers.Adam(learning_rate=0.005))\n",
        "callback = callbacks.EarlyStopping(monitor='loss',patience=2)\n",
        "siamese_model.fit([a_input, p_input,n_input],epochs=10, batch_size = 80,validation_data=[a_input_test[0:1000], p_input_test[0:1000],n_input_test[0:1000]],callbacks=[callback])\n",
        "#We fit our model and run it\n",
        "\n",
        "siamese_model.predict([a_input,p_input,n_input],batch_size=40)  #the output is supposed to be two set of distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHrdS357QgJk"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dblOz19OCD8I"
      },
      "source": [
        "#### Implement a summariser that returns the  n sentences with highest predicted score.\n",
        "\n",
        " - Our code returns the IDs of the  n  sentences that have the highest prediction score in the given question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhXaYArwekJX"
      },
      "outputs": [],
      "source": [
        "def nn_summariser(csvfile, questionids, n=1):\n",
        "  qid_list=questionids   # The input questionids is a list of question ids.\n",
        "  test_2 = pd.read_csv(csvfile)   #input file we want\n",
        "  total=[]\n",
        "  for qid in qid_list:\n",
        "    anchorinput=list(test_2.loc[(test_2[\"qid\"]==qid) & (test_2[\"sentid\"]==0)][\"question\"])  # anchorinput is question with id qid\n",
        "\n",
        "    positiveinput=list(test_2.loc[(test_2[\"qid\"]==qid) & (test_2[\"sentid\"]==0)][\"sentence text\"]) #it doesnt matter and I can fit with random object\n",
        "\n",
        "    negativeinput=(list(test_2.loc[(test_2[\"qid\"]==qid)][\"sentence text\"]))   #these are all answers relaed to this specific question id =qid(including those label 1 or 0)\n",
        "\n",
        "    positiveinput=positiveinput*len(negativeinput)     #we need to increase the length until it matches other inputs\n",
        "    anchorinput=anchorinput*len(negativeinput)         #we need to increase the length until it matches other inputs\n",
        "\n",
        "    a_input=tfidf.transform(anchorinput).toarray()           #We transform those answers and question qid into array\n",
        "    n_input=tfidf.transform(negativeinput).toarray()\n",
        "    p_input=tfidf.transform(positiveinput).toarray()\n",
        "\n",
        "    ap_distance_test,an_distance_test = siamese_model.predict([a_input,p_input,n_input],batch_size=40)\n",
        "    #this an_distance gives us lists of questions with different predicted scores\n",
        "\n",
        "    list(np.argsort(an_distance_test))    #only see one output which is enough in this task and we give the lists of sentence ids with highest predicted score\n",
        "    total.append(list(np.argsort(an_distance_test))[0:n])  #return the list of n answer_index of this question id into the list and append into our final list\n",
        "    #We do for every single qid and give the list of the list\n",
        "  return total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ-Dgkvmfe32",
        "outputId": "2cf53987-4fe9-44de-b34b-ec4b8395ad7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[[24, 28, 25, 7], [40, 45, 44, 38], [0, 5, 3, 6]]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nn_summariser(\"test.csv\",[6,3987,4220],4)  #our testing result  #here we use n=4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR15kL2kCg5o"
      },
      "source": [
        "#### We caluclate the F1 score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBVMn7LLPxvj"
      },
      "outputs": [],
      "source": [
        "#lets make a list of lebals which allow us to caluclate the F1 score in test_score\n",
        "\n",
        "def labels(x,n):\n",
        "    test_3 = pd.read_csv(x)\n",
        "    total_predict=[]\n",
        "    total_true=[]\n",
        "    for qid in list(test_3[\"qid\"].unique())[0:n]:\n",
        "      #uni_id=test_3.loc[(test_3[\"qid\"]==qid)]\n",
        "\n",
        "      anchorinput=list(test_3.loc[(test_3[\"qid\"]==qid) & (test_3[\"sentid\"]==0)][\"question\"])*len(test_3.loc[(test_3[\"qid\"]==qid)])  ##e need to increase the length until it matches other inputs\n",
        "\n",
        "      positiveinput=(list(test_3.loc[(test_3[\"qid\"]==qid)][\"sentence text\"]))   #(positiveinput and negativeinput) can be the same(no difference)  #we use one set of distances only\n",
        "\n",
        "      negativeinput=(list(test_3.loc[(test_3[\"qid\"]==qid)][\"sentence text\"]))\n",
        "\n",
        "      a_input=tfidf.transform(anchorinput).toarray()                       #transform into tfidfvector\n",
        "      n_input=tfidf.transform(negativeinput).toarray()\n",
        "      p_input=tfidf.transform(positiveinput).toarray()\n",
        "\n",
        "\n",
        "      ap_distance_test,an_distance_test = siamese_model.predict([a_input,p_input,n_input],batch_size=40)  #we obtain a set of distances for each answers related to\n",
        "      #the same question\n",
        "      thershold=0.5  #this is out chosen thershold to caluclate the F1 score\n",
        "\n",
        "      y_predict=list(ap_distance_test<thershold)       #predict label 1(0) when the distance is less(more) than thershold\n",
        "\n",
        "      total_predict=total_predict+y_predict            #add the list of predict label for the same question into total_predict\n",
        "      total_true=total_true+list(test_3.loc[(test_3[\"qid\"]==qid)][\"label\"])    #add the list of ground label for the same question into true_total\n",
        "\n",
        "\n",
        "    return (total_predict,total_true)  #turn both value for caluclating F1 score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tt3LWQELg4kM",
        "outputId": "a21f2909-e850-43d6-96e3-8ca105dfeb58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "2/2 [==============================] - 0s 998us/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "2/2 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n"
          ]
        }
      ],
      "source": [
        "total_true_1,total_predict_1=labels(\"test.csv\",50) #We use 50 random datas to calucluate our F1 score.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXJqIoMM-9a6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "f1_score_1=f1_score(total_true_1, total_predict_1) #this is the final F1 score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvBGu5cdCmaw"
      },
      "source": [
        "- We will dicuss our results in the next part when we compare with task 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtZmuIWEhri2"
      },
      "source": [
        "# Task 2 (5 marks): Recurrent NN\n",
        "\n",
        "Implement a more complex Siamese neural network that is composed of the following layers:\n",
        "\n",
        "* An embedding layer that generates embedding vectors of the sentence text with 35 dimensions.\n",
        "* A LSTM layer. You need to determine the size of this LSTM layer, and the text length limit (if needed).\n",
        "* 3 hidden layers and a relu activation function. You need to determine the size of the hidden layers.\n",
        "\n",
        "Train the model with the training data, use the `dev_test` set to determine a good size of the LSTM layer and an appropriate length limit (if needed), and report the final results using the test set. Again, remember to use the test set only after you have determined the optimal parameters of the LSTM layer.\n",
        "\n",
        "Based on your experiments, comment on whether this system is better than the systems developed in the previous tasks.\n",
        "\n",
        "The breakdown of marks is as follows:\n",
        "\n",
        "* **1 mark** if the NN model has the correct layers, the correct activation functions, and the correct loss function.\n",
        "* **1 mark** if the code passes the sentence text to the model correctly. The documentation needs to explain what decisions had to be made to process long sentences. In particular, did you need to truncate the input text, and how did you determine the length limit?\n",
        "* **1 mark** if the code returns the IDs of the *n* sentences that have the highest prediction score in the given question.\n",
        "* **1 mark** if the notebook reports the F1 scores of the test sets and comments on the results.\n",
        "* **1 mark** for good coding and documentation in this task. In particular, the code and results must include evidence that shows your choice of best size of the LSTM layer (and length limit) and hidden layers. The explanations must be clear and concise. To make this task less time-consuming, use $n=1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM8qQESsCvDH"
      },
      "source": [
        "### Creating our tf.idf vector:\n",
        "- We create the tf.idf of triplet data for our train,set and dev_set. Due to the runnning time becomes so low when we add a  LSTM layers and embeeding layers,we decide to use 300 words with highest tfidf score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAYCHFT9kTYY"
      },
      "outputs": [],
      "source": [
        "MAX_LEN=300    #we only use the first MAX_LEN numbers of  higheest tfidf score to train our models\n",
        "#EPOCH=9\n",
        "batch_size = 40    #set our batch size\n",
        "\n",
        "a_input,p_input,n_input=random_triplet_generator('training.csv')  #we created a set of triplet in train,dev_test and test sets\n",
        "a_input_test,p_input_test,n_input_test=random_triplet_generator('dev_test.csv')\n",
        "a_input_test2,p_input_test2,n_input_test2=random_triplet_generator('test.csv')\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf=TfidfVectorizer()\n",
        "\n",
        "tfidf=TfidfVectorizer(stop_words=\"english\",max_features=MAX_LEN)      # We convert into tfidf for each questions and answers\n",
        "\n",
        "tfidf.fit(a_input+p_input+n_input)  #we fit the tfidf\n",
        "\n",
        "\n",
        "a_input=tfidf.transform(a_input).toarray()\n",
        "n_input=tfidf.transform(n_input).toarray()       #convert our questions and answers triplet into tfidf vectors\n",
        "p_input=tfidf.transform(p_input).toarray()\n",
        "\n",
        "a_input_test=tfidf.transform(a_input_test).toarray()\n",
        "n_input_test=tfidf.transform(n_input_test).toarray()\n",
        "p_input_test=tfidf.transform(p_input_test).toarray()\n",
        "\n",
        "a_input_test2=tfidf.transform(a_input_test2).toarray()\n",
        "n_input_test2=tfidf.transform(n_input_test2).toarray()\n",
        "p_input_test2=tfidf.transform(p_input_test2).toarray()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l0ti0LoC8Ho"
      },
      "source": [
        "#### We create our model with the requirement:\n",
        "-  An embedding layer that generates embedding vectors of the sentence text with 35 dimensions.\n",
        "- A LSTM layer.we set the contraint of the input text with 300.\n",
        "- 3 hidden layers and a relu activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzmFmMLUu2ry",
        "outputId": "e9bffa3d-c15d-498b-f972-ad4ae004ee75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_17\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_58 (InputLayer)          [(None, 300)]        0           []                               \n",
            "                                                                                                  \n",
            " input_59 (InputLayer)          [(None, 300)]        0           []                               \n",
            "                                                                                                  \n",
            " input_60 (InputLayer)          [(None, 300)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 300, 35)      10535       ['input_58[0][0]',               \n",
            "                                                                  'input_59[0][0]',               \n",
            "                                                                  'input_60[0][0]']               \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 36)           10368       ['embedding_1[0][0]',            \n",
            "                                                                  'embedding_1[1][0]',            \n",
            "                                                                  'embedding_1[2][0]']            \n",
            "                                                                                                  \n",
            " sequential_17 (Sequential)     (None, 26)           2366        ['lstm[0][0]',                   \n",
            "                                                                  'lstm[1][0]',                   \n",
            "                                                                  'lstm[2][0]']                   \n",
            "                                                                                                  \n",
            " triplet_loss_layer (TripletLos  ((None,),           0           ['sequential_17[0][0]',          \n",
            " sLayer)                         (None,))                         'sequential_17[1][0]',          \n",
            "                                                                  'sequential_17[2][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23,269\n",
            "Trainable params: 23,269\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "from keras.models import Model, Sequential\n",
        "\n",
        "from keras.layers import  Input, Embedding, LSTM, Bidirectional, Dense, Lambda\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "#we create the input with the max_len input shape to fit our model\n",
        "\n",
        "# add three layers(hidden one)\n",
        "\n",
        "def siamese_network_generator(x,y,z,w):  #we create a function for building the siamese_network with 4 inputs of the hidden/LSTM layers\n",
        "    embed_size=35\n",
        "    first_sent_in_a = Input(shape=(MAX_LEN,))\n",
        "    second_sent_in_p = Input(shape=(MAX_LEN,))\n",
        "    third_sent_in_n = Input(shape=(MAX_LEN,))\n",
        "\n",
        "    embedding_layer =  Embedding(input_dim=MAX_LEN+1, output_dim=embed_size, input_length=MAX_LEN, trainable=True)\n",
        "#An embedding layer that generates embedding vectors of the sentence text with 35 dimensions.\n",
        "#Max_len +1 : we might have some known words which are not in dataset.\n",
        "\n",
        "\n",
        "    simpleLSTM = LSTM(units=w, return_sequences=False) #w is the units of LSTM\n",
        "#A LSTM layer. You need to determine the size of this LSTM layer, and the text length limit (if needed).\n",
        "\n",
        "\n",
        "    first_sent_encoded_a = simpleLSTM(embedding_layer(first_sent_in_a) )  #get representation for first sentense\n",
        "    second_sent_encoded_p = simpleLSTM(embedding_layer(second_sent_in_p))\n",
        "    third_sent_encoded_n = simpleLSTM(embedding_layer(third_sent_in_n))\n",
        "\n",
        "\n",
        "    model = Sequential([\n",
        "      Dense(x, activation='relu'),                        #we have three hidden layers into the model\n",
        "        Dense(y, activation='relu'),\n",
        "         Dense(z, activation='relu')\n",
        "    ])\n",
        "\n",
        "    encoded_1 = model(first_sent_encoded_a )\n",
        "    encoded_2 = model(second_sent_encoded_p)    #these are the output where the datas passed through 3 hidden layers\n",
        "    encoded_3 = model(third_sent_encoded_n)\n",
        "\n",
        "\n",
        "    class TripletLossLayer(Layer):              #create a distance layer as required\n",
        "        def __init__(self, **kwargs):\n",
        "            super().__init__(**kwargs)\n",
        "\n",
        "        def call(self, anchor, positive, negative):\n",
        "            ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
        "            an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)         #we need inheritance from the superclass layer.\n",
        "            return (ap_distance, an_distance)\n",
        "        #we caluclate the distance between the lists of positive(negative) and anchor vector\n",
        " #Returns the squared Euclidean distance between anchor and positive answer, as well as that between anchor and negative answer\n",
        "    loss_layer = TripletLossLayer()(encoded_1, encoded_2, encoded_3)\n",
        "\n",
        "    siamese_network = Model([first_sent_in_a, second_sent_in_p, third_sent_in_n], loss_layer)\n",
        "\n",
        "    return siamese_network\n",
        " #we define our siamese_network here\n",
        "\n",
        "\n",
        "class LSTMSiameseModel(Model):\n",
        "# next we define our custom model class as our task is not traditional classification task\n",
        "#there are lot of thing needed to be changed\n",
        "\n",
        "    def __init__(self, siamese_network, margin=0.000000000000001):\n",
        "        super().__init__()\n",
        "        self.siamese_network = siamese_network   # We create a build and create those object\n",
        "        self.margin = margin                     #We define our own metrics with the loss function \"triplet_lost\" we define below\n",
        "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.siamese_network(inputs) # #this is the margin value defined for our triplet loss\n",
        "    #@tf.function\n",
        "    def train_step(self, data):\n",
        "\n",
        "        with tf.GradientTape() as tape: #we define our custom training step with trainable_weights\n",
        "            loss = self.triplet_lost(data)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)  #we use gradient descent to keep track of each steps\n",
        "\n",
        "        self.optimizer.apply_gradients(\n",
        "            zip(gradients, self.siamese_network.trainable_weights)\n",
        "        )\n",
        "\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}   # return the loss for each steps\n",
        "    #@tf.function\n",
        "    def test_step(self, data):            #we do the same thing for test step\n",
        "        loss = self.triplet_lost(data)\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss_3\": self.loss_tracker.result()}\n",
        "    #@tf.function\n",
        "    def triplet_lost(self, data):         #the loss function loss_3 is for triplet validation dataset or test set (depended on situation)\n",
        "\n",
        "\n",
        "        ap_distance, an_distance = self.siamese_network(data)   # we defined the triplet lost fucntion inside our custom fit\n",
        "\n",
        "        loss=(tf.maximum(ap_distance - an_distance + self.margin, 0.0))\n",
        "\n",
        "\n",
        "        return loss\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "                                       #we need our loss tracker\n",
        "        return [self.loss_tracker]\n",
        "\n",
        "siamese_network=siamese_network_generator(26,26,26,36)\n",
        "siamese_network.summary()                       #print out the detail of model we created\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2jIiXaODqJm"
      },
      "source": [
        "#### Finding the best hyperparamters\n",
        "\n",
        "- In this part we search for different input of parameters and find the best paramters for least validation loss\n",
        "\n",
        "- to simplify the process,we choose different hidden sizes to test the performance but we use the same number of hidden sizes in all 3 hidden layers and we set a fix learning rate with a small margin value(because the validation lost didnt go down after lone period)\n",
        "\n",
        "- the w paramters in validation_evaluation is our LSTM hidden sizes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixn8KEvdCZLQ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import callbacks\n",
        "# we set a  function evluate the validation loss by puting different hyperparamters\n",
        "def validation_evaluation_(num_1,num_2,num_3,learning_rate,alpha,w):   #this function gives a input of parameters for each layers,learning rate and margin values\n",
        "  siamese_model = LSTMSiameseModel(siamese_network_generator(num_1,num_2,num_3,w),alpha)\n",
        "  siamese_model.compile(optimizer=optimizers.Adam(learning_rate))\n",
        "  callback = callbacks.EarlyStopping(monitor='loss',patience=2)\n",
        "\n",
        "  siamese_model.fit([a_input, p_input,n_input],epochs=10, batch_size = 80,validation_data=[a_input_test[0:1000], p_input_test[0:1000],n_input_test[0:1000]],callbacks=[callback])\n",
        "  return (siamese_model.evaluate([a_input_test,p_input_test,n_input_test],batch_size = 80),[num_1,num_2,num_3,learning_rate,alpha]) #the output is a the evaluation score\n",
        "  # we use 1000 data (approxmiate to be 1/10 of our our dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xN3CxVMov1tQ",
        "outputId": "6020b75f-9f39-4302-9cf2-08231735bf6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 1.6104e-09WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 22s 130ms/step - loss: 1.6104e-09 - val_loss_3: 7.2807e-11\n",
            "Epoch 2/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 1.4789e-09WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 12s 83ms/step - loss: 1.4789e-09 - val_loss_3: 8.0660e-10\n",
            "Epoch 3/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 1.0844e-09WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 12s 82ms/step - loss: 1.0844e-09 - val_loss_3: 7.5455e-11\n",
            "Epoch 4/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 2.5434e-10WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 12s 84ms/step - loss: 2.5434e-10 - val_loss_3: 1.2454e-11\n",
            "Epoch 5/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 3.8459e-11WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 12s 84ms/step - loss: 3.8459e-11 - val_loss_3: 7.0685e-12\n",
            "Epoch 6/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 1.2017e-11WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 12s 81ms/step - loss: 1.2017e-11 - val_loss_3: 5.6773e-12\n",
            "Epoch 7/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 8.3344e-12WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 12s 84ms/step - loss: 8.3344e-12 - val_loss_3: 5.2354e-12\n",
            "Epoch 8/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 6.7443e-12WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 12s 86ms/step - loss: 6.7443e-12 - val_loss_3: 5.0715e-12\n",
            "Epoch 9/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 5.9690e-12WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 12s 82ms/step - loss: 5.9690e-12 - val_loss_3: 5.0226e-12\n",
            "Epoch 10/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 5.6060e-12WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 13s 89ms/step - loss: 5.6060e-12 - val_loss_3: 5.0139e-12\n",
            "49/49 [==============================] - 2s 27ms/step - loss_3: 5.0015e-12\n",
            "Epoch 1/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 8.6884e-09WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 32s 203ms/step - loss: 8.6884e-09 - val_loss_3: 1.2843e-09\n",
            "Epoch 2/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 7.2541e-10WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 26s 178ms/step - loss: 7.2541e-10 - val_loss_3: 1.7913e-09\n",
            "Epoch 3/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 9.2290e-09WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 25s 175ms/step - loss: 9.2290e-09 - val_loss_3: 5.9713e-10\n",
            "Epoch 4/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 1.3770e-09WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 25s 177ms/step - loss: 1.3770e-09 - val_loss_3: 4.9646e-09\n",
            "49/49 [==============================] - 4s 56ms/step - loss_3: 4.0637e-09\n",
            "Epoch 1/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 6.2065e-09WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 31s 191ms/step - loss: 6.2065e-09 - val_loss_3: 3.8917e-10\n",
            "Epoch 2/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 2.9805e-09WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 20s 136ms/step - loss: 2.9805e-09 - val_loss_3: 4.1087e-09\n",
            "Epoch 3/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 1.1490e-08WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 19s 136ms/step - loss: 1.1490e-08 - val_loss_3: 1.4841e-10\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "143/143 [==============================] - ETA: 0s - loss: 7.9354e-09WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 19s 131ms/step - loss: 7.9354e-09 - val_loss_3: 1.2781e-10\n",
            "49/49 [==============================] - 2s 31ms/step - loss_3: 1.0964e-10\n",
            "Epoch 1/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 1.3744e-08WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 28s 173ms/step - loss: 1.3744e-08 - val_loss_3: 2.0267e-10\n",
            "Epoch 2/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 6.2857e-09WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 23s 162ms/step - loss: 6.2857e-09 - val_loss_3: 3.6003e-09\n",
            "Epoch 3/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 4.9745e-09WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 23s 163ms/step - loss: 4.9745e-09 - val_loss_3: 1.0914e-10\n",
            "Epoch 4/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 1.0155e-08WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 23s 164ms/step - loss: 1.0155e-08 - val_loss_3: 4.2470e-11\n",
            "Epoch 5/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 4.0347e-08WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 28s 199ms/step - loss: 4.0347e-08 - val_loss_3: 1.7114e-08\n",
            "49/49 [==============================] - 7s 121ms/step - loss_3: 1.3893e-08\n",
            "Epoch 1/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 1.1393e-08WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 34s 214ms/step - loss: 1.1393e-08 - val_loss_3: 6.4281e-09\n",
            "Epoch 2/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 9.2724e-09WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 28s 193ms/step - loss: 9.2724e-09 - val_loss_3: 6.5888e-10\n",
            "Epoch 3/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 9.7651e-10WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 27s 192ms/step - loss: 9.7651e-10 - val_loss_3: 9.6195e-10\n",
            "Epoch 4/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 2.3203e-09WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 28s 199ms/step - loss: 2.3203e-09 - val_loss_3: 3.3667e-08\n",
            "Epoch 5/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 2.6669e-08WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 28s 196ms/step - loss: 2.6669e-08 - val_loss_3: 4.3852e-09\n",
            "49/49 [==============================] - 4s 66ms/step - loss_3: 3.6722e-09\n",
            "Epoch 1/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 6.3579e-09WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 53s 348ms/step - loss: 6.3579e-09 - val_loss_3: 6.2130e-09\n",
            "Epoch 2/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 1.3791e-08WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 45s 315ms/step - loss: 1.3791e-08 - val_loss_3: 8.2390e-10\n",
            "Epoch 3/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 1.1527e-08WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 45s 318ms/step - loss: 1.1527e-08 - val_loss_3: 1.7820e-09\n",
            "49/49 [==============================] - 6s 117ms/step - loss_3: 1.4422e-09\n"
          ]
        }
      ],
      "source": [
        "# we have three trial with different hyperparameters\n",
        "score_list=[]                         # We want to see determine best performance of each model\n",
        "for i in [16,32,64]:    #to be simple: I only use the same hyperparamters for each layers and we use different neurons w for LSTM layers\n",
        "    for w in [16,32]:  #we choose a smaller values due to memory problem\n",
        "        score_list.append((validation_evaluation_(i,i,i,0.005,0.000000000005,w),i,w)) #set very small margins due to higher one gives no result #we just make it simple by looking at\n",
        "  #if ap_distance is smaller than an_distance only\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fW4LEvBdXb0N"
      },
      "outputs": [],
      "source": [
        "scored_list=[]\n",
        "for i ,j ,k in score_list:\n",
        "    scored_list.append((i[0],i[1][0],k))  #to make a clear list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLjJT1KJXb0N",
        "outputId": "8144c541-20f1-424d-a345-e37b62098277"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(4.993807884573487e-12, 16, 16),\n",
              " (3.509061485829079e-09, 16, 32),\n",
              " (9.369448966678462e-11, 32, 16),\n",
              " (1.2203935639831798e-08, 32, 32),\n",
              " (3.1017435286884165e-09, 64, 16),\n",
              " (1.2703317286977267e-09, 64, 32)]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scored_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WE5BjNQjXb0N",
        "outputId": "49b003ec-a6ef-49e5-8afd-dfcaef774363"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best_hidden parameters: 16 16 16\n",
            "best_LSTM units: 16\n"
          ]
        }
      ],
      "source": [
        "minimum_lost=100000\n",
        "\n",
        "for i , j ,w in (scored_list):\n",
        "  if i<minimum_lost:\n",
        "    best_praramters=(j,w)\n",
        "    minimum_lost=i      # we search for the best minimum losts with the best_praramters\n",
        "\n",
        "print (\"best_hidden parameters:\",best_praramters[0],best_praramters[0],best_praramters[0])\n",
        "\n",
        "print (\"best_LSTM units:\",best_praramters[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOxH5FZdDx3O"
      },
      "source": [
        "#### we retrain the model using best hyperparameters and make prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAa6kDRtEGnM",
        "outputId": "5034b786-ed5f-4940-d0ee-f286b737101d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 1.0178e-08WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 43s 274ms/step - loss: 1.0178e-08 - val_loss_3: 1.2687e-08\n",
            "Epoch 2/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 6.4221e-09WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 39s 274ms/step - loss: 6.4221e-09 - val_loss_3: 2.1514e-11\n",
            "Epoch 3/10\n",
            "143/143 [==============================] - ETA: 0s - loss: 1.3160e-08WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
            "143/143 [==============================] - 38s 269ms/step - loss: 1.3160e-08 - val_loss_3: 8.4988e-10\n",
            "286/286 [==============================] - 12s 38ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(array([0.0000000e+00, 3.0253577e-15, 2.0832516e-08, ..., 0.0000000e+00,\n",
              "        0.0000000e+00, 1.0580784e-07], dtype=float32),\n",
              " array([0.0000000e+00, 4.8017146e-15, 2.0832516e-08, ..., 0.0000000e+00,\n",
              "        0.0000000e+00, 1.0579440e-07], dtype=float32))"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "minimum_lost=100000\n",
        "\n",
        "for i , j ,w in (scored_list):\n",
        "  if i<minimum_lost:\n",
        "    best_praramters=(j,w)\n",
        "    minimum_lost=i      # we search for the best minimum losts with the best_praramters\n",
        "\n",
        "best_praramters #this is the one we obatin\n",
        "LSTMsiamese_model=LSTMSiameseModel(siamese_network_generator(best_praramters[0],best_praramters[0],best_praramters[0],best_praramters[1]),0.000000000005)\n",
        "\n",
        "LSTMsiamese_model.compile(optimizer=optimizers.Adam(learning_rate=0.005))\n",
        "callback = callbacks.EarlyStopping(monitor='loss',patience=1)\n",
        "LSTMsiamese_model.fit([a_input, p_input,n_input],epochs=10, batch_size = 80,validation_data=[a_input_test[0:1000], p_input_test[0:1000],n_input_test[0:1000]],callbacks=[callback])\n",
        "\n",
        "\n",
        "\n",
        "#We fit our model and run it\n",
        "\n",
        "LSTMsiamese_model.predict([a_input,p_input,n_input],batch_size=40)  #the output is supposed to be two set of distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFQH3y9slQGs",
        "outputId": "c5e4b535-a07b-44e3-f3f2-772ffe74c6ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "97/97 [==============================] - 4s 40ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(array([5.6066263e-15, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
              "        0.0000000e+00, 0.0000000e+00], dtype=float32),\n",
              " array([5.6066263e-15, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
              "        0.0000000e+00, 0.0000000e+00], dtype=float32))"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "LSTMsiamese_model.predict([a_input_test, p_input_test,n_input_test],batch_size=40)  #some test inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZe5HtnLD1Hm"
      },
      "source": [
        "#### Implement a LSTMnn_summariser that returns the  n sentences with highest predicted score.\n",
        "\n",
        " - Our code returns the IDs of the  n  sentences that have the highest prediction score in the given question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-BXol95PIi2"
      },
      "outputs": [],
      "source": [
        "def LSTMnn_summariser(csvfile, questionids, n=1):\n",
        "  qid_list=questionids\n",
        "  test_2 = pd.read_csv(csvfile)\n",
        "  total=[]\n",
        "  for qid in qid_list:\n",
        "    anchorinput=list(test_2.loc[(test_2[\"qid\"]==qid) & (test_2[\"sentid\"]==0)][\"question\"])  #it doesnt matter and I can fit with random object\n",
        "\n",
        "    positiveinput=list(test_2.loc[(test_2[\"qid\"]==qid) & (test_2[\"sentid\"]==0)][\"sentence text\"])\n",
        "    #these are all answers relaed to this specific question id =qid(including those label 1 or 0)\n",
        "\n",
        "    negativeinput=(list(test_2.loc[(test_2[\"qid\"]==qid)][\"sentence text\"]))\n",
        "\n",
        "    positiveinput=positiveinput*len(negativeinput)   #we need to increase the length until it matches other inputs\n",
        "    anchorinput=anchorinput*len(negativeinput)       #we need to increase the length until it matches other inputs\n",
        "\n",
        "    a_input=tfidf.transform(anchorinput).toarray()  #We transform those answers and question qid into array\n",
        "    n_input=tfidf.transform(negativeinput).toarray()\n",
        "    p_input=tfidf.transform(positiveinput).toarray()\n",
        "\n",
        "    ap_distance_test,an_distance_test = LSTMsiamese_model.predict([a_input,p_input,n_input],batch_size=40)\n",
        "     #this an_distance gives us lists of questions with different predicted scores\n",
        "    #only see one output which is enough in this task and we give the lists of sentence ids with highest predicted score\n",
        "\n",
        "    total.append(list(np.argsort(an_distance_test))[0:n])  #return the list of answer_index which has minimums\n",
        "    #return the list of n answer_index of this question id into the list and append into our final list\n",
        "\n",
        "  return total #We do for every single qid and give the list of the list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJQoL3YHvG2s",
        "outputId": "aae436c3-4f10-46d7-fc65-decdc0dbd359"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 51ms/step\n",
            "2/2 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[[0, 29, 28, 27], [0, 27, 28, 29], [0, 1, 2, 3], [0, 1, 4, 5]]"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "LSTMnn_summariser(\"test.csv\",[6,3987,4220,7],4) #example n=4 only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIl3OyKAXb0O",
        "outputId": "70a40e44-8f26-408b-8d64-8c6780e42080"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 47ms/step\n",
            "2/2 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[[0], [0], [0], [0]]"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "LSTMnn_summariser(\"test.csv\",[6,3987,4220,7],1) #example n=1 only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ9Q_h3DOlS3"
      },
      "outputs": [],
      "source": [
        "def LSTMlabels(x,n):\n",
        "    test_3 = pd.read_csv(x)\n",
        "    total_predict=[]\n",
        "    total_true=[]\n",
        "    for qid in list(test_3[\"qid\"].unique())[0:n]:\n",
        "      #uni_id=test_3.loc[(test_3[\"qid\"]==qid)]\n",
        "\n",
        "      anchorinput=list(test_3.loc[(test_3[\"qid\"]==qid) & (test_3[\"sentid\"]==0)][\"question\"])*len(test_3.loc[(test_3[\"qid\"]==qid)])\n",
        "\n",
        "      positiveinput=(list(test_3.loc[(test_3[\"qid\"]==qid)][\"sentence text\"]))   #they can be the same-no difference  #we use one set of distances only\n",
        "\n",
        "      negativeinput=(list(test_3.loc[(test_3[\"qid\"]==qid)][\"sentence text\"]))\n",
        "\n",
        "      a_input=tfidf.transform(anchorinput).toarray()   #\n",
        "      n_input=tfidf.transform(negativeinput).toarray()\n",
        "      p_input=tfidf.transform(positiveinput).toarray()\n",
        "\n",
        "\n",
        "      ap_distance_test,an_distance_test = LSTMsiamese_model.predict([a_input,p_input,n_input],batch_size=40)\n",
        "      thershold=0.5\n",
        "\n",
        "      y_predict=list(ap_distance_test<thershold)       #predict label 1 when the distance is less than thershold\n",
        "\n",
        "      total_predict=total_predict+y_predict            # add into the final list of the predict label of id= qid\n",
        "      total_true=total_true+list(test_3.loc[(test_3[\"qid\"]==qid)][\"label\"])    #add the list of true label related to id= qid\n",
        "\n",
        "\n",
        "    return (total_predict,total_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cuu3-Dg09YHx",
        "outputId": "5fd1497c-7b61-4aca-c964-33ca80c8829a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "test_2 = pd.read_csv(\"test.csv\")\n",
        "\n",
        "\n",
        "#lets make a list of lebals which allow us to caluclate the F1 score in test_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "total_true,total_predict=LSTMlabels(\"test.csv\",30)  #second paramter: use smaller number of test set data to do F1 score\n",
        "\n",
        "f1_score_2=f1_score(total_true, total_predict)\n",
        "\n",
        "#f1_score_1\n",
        "f1_score_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_gvceRMECEl"
      },
      "source": [
        "- We plot the graph below and give an analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "byCM7CtIKmoK",
        "outputId": "92c206e8-af6c-432e-b098-73a174b94e42"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# creating the dataset\n",
        "data = {'Simplemodel_1':f1_score_1, 'LSTMCmodel_2':f1_score_2}\n",
        "keys = list(data.keys())\n",
        "values = list(data.values())\n",
        "\n",
        "fig = plt.figure(figsize = (10, 5))\n",
        "axes = plt.axes()\n",
        "axes.set_ylim([0.2, 0.9])\n",
        "# creating the bar plot\n",
        "plt.bar(keys, values\n",
        "        )\n",
        "\n",
        "plt.title('F1 for task 1 VS F1 for task 2')\n",
        "plt.xlabel(\"Classifier\")\n",
        "plt.ylabel(\"The F1 of accuracy\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBX7DjZdXb0P",
        "outputId": "4abae019-5c81-4ccf-f36d-e338ad2804dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Our F1_score in simple model : 0.39581351094196005\n",
            "Our F1_score in LSTMCmodel : 0.37387964148527525\n"
          ]
        }
      ],
      "source": [
        "print (\"Our F1_score in simple model :\",f1_score_1)  #report F1 scores for both tasks\n",
        "print (\"Our F1_score in LSTMCmodel :\",f1_score_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eTAcSFAJapv"
      },
      "source": [
        "### Comment in case 1 and case 2:\n",
        "\n",
        "- From the graph,We see the performance in both cases didnt performing well as we can see the F1 scores in both models .Due to the limit of the colab and the notebook structure,the memory will lost very quickly if I use too mnay hyperparamters or use too many inputs.I think this is the main reason why the performace is low.It is hard to turn thoese hyperparamters because the sped will become very slow and it is not feasible to do it.\n",
        "\n",
        "- In Case 1,it looks like that the model is too simple and it cannot capture the pattern.\n",
        "\n",
        "- In case 2,it look like we cannot find the good hyperparamters within limited resource and time.\n",
        "\n",
        "- Thershold of F1 score is not easy to choose:it seems it is unlikely we can plug a thershold =0.5(as suggested by some literature) and caluclate the result since the distances structure have complicated relationship (for example if all ap_distance and np_distance are greater less than 0.5,then there is no point to choose such F1 score thershold. Also the distances might depends on the margins values in lost function).In order to explore F1 score,we need to have more time but We have two weeks to do these task which makes this hard to acheieve.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gft_D1A6mhLE"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSlPG7MN8Qws"
      },
      "source": [
        "# Task 3 (5 marks): Transformer\n",
        "\n",
        "Implement a simple Transformer neural network that is composed of the following layers:\n",
        "\n",
        "* Use BERT as feature extractor for each token.\n",
        "* A few of transformer encoder layers, hidden dimension 768. You need to determine how many layers to use between 1~3.\n",
        "* A few of transformer decoder layers, hidden dimension 768. You need to determine how many layers to use between 1~3.\n",
        "* 1 hidden layer with size 512.\n",
        "* The final output layer with one cell for binary classification to predict whether two inputs are related or not.\n",
        "\n",
        "Note that each input for this model should be a concatenation of a positive pair (i.e. question + one answer) or a negative pair (i.e. question + not related sentence). The format is usually like [CLS]+ question + [SEP] + a positive/negative sentence.\n",
        "\n",
        "Train the model with the training data, use the dev_test set to determine a good size of the transformer layers, and report the final results using the test set. Again, remember to use the test set only after you have determined the optimal parameters of the transformer layers.\n",
        "\n",
        "Based on your experiments, comment on whether this system is better than the systems developed in the previous tasks.\n",
        "\n",
        "The breakdown of marks is as follows:\n",
        "\n",
        "* **1 mark** if the model has the correct layers, the correct activation functions, and the correct loss function.\n",
        "* **1 mark** if the code passes the sentence text to the model correctly. The documentation needs to explain how to handle length difference for a batch of data\n",
        "* **1 mark** if the code returns the IDs of the *n* sentences that have the highest prediction score in the given question.\n",
        "* **1 mark** if the notebook reports the F1 scores of the test sets and comments on the results.\n",
        "* **1 mark** for good coding and documentation in this task. In particular, the code and results must include evidence that shows your choice of best size of the transformer layers. The explanations must be clear and concise. To make this task less time-consuming, use $n=1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKSi6k-dqOdB",
        "outputId": "87f269ce-a349-4698-bd84-69da3bde3482"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.41.1-py3-none-any.whl (9.1 MB)\n",
            "     ---------------------------------------- 9.1/9.1 MB 29.1 MB/s eta 0:00:00\n",
            "Collecting safetensors>=0.4.1\n",
            "  Downloading safetensors-0.4.3-cp39-none-win_amd64.whl (287 kB)\n",
            "     ------------------------------------- 287.9/287.9 kB 17.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\mqu\\sci\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.23.0\n",
            "  Downloading huggingface_hub-0.23.1-py3-none-any.whl (401 kB)\n",
            "     ---------------------------------------- 401.3/401.3 kB ? eta 0:00:00\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\mqu\\sci\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in c:\\mqu\\sci\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\mqu\\sci\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
            "Collecting tokenizers<0.20,>=0.19\n",
            "  Downloading tokenizers-0.19.1-cp39-none-win_amd64.whl (2.2 MB)\n",
            "     ---------------------------------------- 2.2/2.2 MB 47.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: requests in c:\\mqu\\sci\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\mqu\\sci\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\mqu\\sci\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\mqu\\sci\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.3.0)\n",
            "Collecting fsspec>=2023.5.0\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "     ------------------------------------- 316.1/316.1 kB 20.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\mqu\\sci\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: colorama in c:\\mqu\\sci\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\mqu\\sci\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\mqu\\sci\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\mqu\\sci\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\mqu\\sci\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
            "Installing collected packages: safetensors, fsspec, huggingface-hub, tokenizers, transformers\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2022.7.1\n",
            "    Uninstalling fsspec-2022.7.1:\n",
            "      Successfully uninstalled fsspec-2022.7.1\n",
            "Successfully installed fsspec-2024.5.0 huggingface-hub-0.23.1 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.41.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qdY1KePEae-"
      },
      "source": [
        "#### Use BERT as feature extractor for each token\n",
        "\n",
        "- in this part we create a dataset for question+ answers pair and label\n",
        "- we convert into Bert_dataset format and use BertTokenizer to get out inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRA9948OcI6i"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "\n",
        "def Bert_dataset(x,num):                             #we create a Bert_dataset to do this part: ach input for this model should be a concatenation of a positive pair (i.e. question + one answer)\n",
        "#or a negative pair (i.e. question + not related sentence). The format is usually like [CLS]+ question + [SEP] + a positive/negative sentence\n",
        "#num is the number of random sample we want\n",
        "  data=pd.read_csv(x).sample(num)\n",
        "  question_input=data[\"question\"].to_list()\n",
        "  answers_input=data[\"sentence text\"].to_list()\n",
        "#The format is usually like [CLS]+ question + [SEP] + a positive/negative sentence\n",
        "\n",
        "  question_encoded= [\"[CLS]\"+\" \"+x+\" \"+\"[SEP]\"+\" \" for x in question_input]\n",
        "\n",
        "  answers_encoded= [x+\" \"+\"[SEP]\" for x in answers_input]\n",
        "\n",
        "  answers_encoded= [x+\" \"+\"[SEP]\" for x in answers_input]\n",
        "  w=[]\n",
        "  for last, first in zip(question_encoded, answers_encoded):\n",
        "    w.append(last+first)\n",
        "#We also write out our label\n",
        "  label=data[\"label\"].to_list()\n",
        "\n",
        "\n",
        "\n",
        "  return (w,label)\n",
        "#we use a function Bert_dataset to return the pair of random X and Y_label we need\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cY9qmAxN7Rk8",
        "outputId": "2f154340-7a91-46f0-f1d6-7f7b859603bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['[CLS] What is marked by DNaseI hypersensitive sites? [SEP] BACKGROUND: Mapping DNaseI hypersensitive sites is commonly used to identify regulatory regions in the genome. [SEP]',\n",
              "  '[CLS] Which genes have been proposed as potential candidates for gene therapy of heart failure? [SEP] therapeutic activation of YAP or its downstream targets, potentially through AAV-mediated gene therapy, may be a strategy to improve outcome after MI [SEP]',\n",
              "  '[CLS] Is there an association between FGFR3 mutation and plagiocephaly? [SEP] Between January and December of 1996, patients with a diagnosis of plagiocephaly at the Children&apos;s Hospital of Philadelphia were evaluated for the FGFR3 mutation [SEP]',\n",
              "  '[CLS] What is the inheritance of hypophosphatemic rickets? [SEP] The study of this family tree strongly suggests an x-linked dominant inheritance. [SEP]',\n",
              "  \"[CLS] What is the function of Oseltamivir when administered during flu? [SEP] Oseltamivir (has known by its brand name 'Tamiflu') is a prodrug, requiring ester hydrolysis for conversion to the active form, Oseltamivir carboxylate. [SEP]\",\n",
              "  '[CLS] What is the gold standard treatment for Iatrogenic male incontinence? [SEP] The initial treatment for SUI that persists after 12 months consists of conservative measures such as pelvic floor muscle exercises and behavioral therapy. [SEP]',\n",
              "  '[CLS] Velocardial facial syndrome, otherwise known as Di George syndrome  is caused by a deletion in chromosome 21, yes or no? [SEP] [Microdeletion of the chromosome 22q11 in children: apropos of a series of 49 patients].<AbstractText Label=\"UNLABELLED\">Most of the children with Di George syndrome and 60% of patients with velocardiofacial syndrome exhibit a microdeletion within chromosome 22q11. [SEP]'],\n",
              " [1, 0, 0, 0, 1, 0, 1])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Bert_dataset(\"training.csv\",7) #testing example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DS9wzvW8ZJD"
      },
      "outputs": [],
      "source": [
        "train_pairs, train_labels = Bert_dataset('training.csv',5000)   #due to the limit of the memory and other tech issues,we use 5000 datas to train the model only\n",
        "dev_test_pairs, dev_test_labels = Bert_dataset('dev_test.csv',500)  #create the pair of positive,negative and anchor\n",
        "test_pairs, test_labels = Bert_dataset('test.csv',500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nicMrL979qQ2"
      },
      "outputs": [],
      "source": [
        "def BertTokenizer_pairs(x, MAX_LEN):                           #we need to tokenize them using Berttokenizer\n",
        "    Berttokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    output = Berttokenizer(x, return_tensors='tf', max_length=MAX_LEN, padding=True, truncation=True)  #to handle different batch of data,we us padding and truncation\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqEbLcvZ9qgG"
      },
      "outputs": [],
      "source": [
        "MAX_LEN=128  #we set the len to be 128 for the max length for Berttokenizer\n",
        "\n",
        "#these are out input of three sets of data we need\n",
        "\n",
        "train_inputs = BertTokenizer_pairs(train_pairs, MAX_LEN)\n",
        "dev_test_inputs = BertTokenizer_pairs(dev_test_pairs, MAX_LEN)\n",
        "test_inputs = BertTokenizer_pairs(test_pairs, MAX_LEN)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgzHjQVGEwSF"
      },
      "source": [
        "#### This is the model and part of it I generate from Ai:\n",
        "\n",
        "- We build up a TransformerEncoderLayer and TransformerDecoderLayer subclass\n",
        "\n",
        "- Next we build up the whole model which fits the requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMliPOW9f7ya"
      },
      "outputs": [],
      "source": [
        "# Custom Transformer layers\n",
        "class TransformerEncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_dim, num_heads, ff_dim):\n",
        "        super(TransformerEncoderLayer, self).__init__()  #super (): we inherit from othe super class layer.\n",
        "        self.attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=hidden_dim)  #we set the MultiHeadAttention head with hidden dimention\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(ff_dim, activation='relu'),  #we have to create a feed forward loop and ff_dim are the dimentions\n",
        "            tf.keras.layers.Dense(hidden_dim),\n",
        "            tf.keras.layers.Dense(hidden_dim),\n",
        "        ])\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)  #these are the LayerNormalization\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        attn_output = self.attention(inputs, inputs)  #We let our pair of same inputs go into attention layers and it goes into the first LayerNormalization\n",
        "\n",
        "\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        #We let our pair of same inputs go into attention layers and it goes into the first LayerNormalization\n",
        "        ffn_output = self.ffn(out1)\n",
        "        # we feed into feedforward layer\n",
        "        return self.layernorm2(out1 + ffn_output)  # feed the last output from attention layers with the output from feedforward layer to get final output\n",
        "\n",
        "class TransformerDecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_dim, num_heads, ff_dim):\n",
        "        super(TransformerDecoderLayer, self).__init__()  #super (): we inherit from othe super class layer.\n",
        "        self.attention1 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=hidden_dim)\n",
        "        self.attention2 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=hidden_dim)  #we set the MultiHeadAttentions head with hidden dimention\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(ff_dim, activation='relu'), #we have to create a feed forward loop and ff_dim are the dimentions\n",
        "            tf.keras.layers.Dense(hidden_dim),\n",
        "        ])\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6) #these are the LayerNormalization\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "\n",
        "    def call(self, inputs, enc_outputs):\n",
        "        attn1 = self.attention1(inputs, inputs)   #We let our pair of same inputs go into attention layers and it goes into the first LayerNormalization\n",
        "\n",
        "        out1 = self.layernorm1(inputs + attn1)  #We let our pair of same inputs go into attention layers and it goes into the first LayerNormalization\n",
        "        attn2 = self.attention2(out1, enc_outputs)   #we use our output in encoding layer and feed into second self-attention layer\n",
        "\n",
        "        out2 = self.layernorm2(out1 + attn2)  #combine together and give output\n",
        "        ffn_output = self.ffn(out2)   #pass through the feedforword loop\n",
        "\n",
        "        return self.layernorm3(out2 + ffn_output)  # feed the last out2 with the output from feedforward layer to get final output\n",
        "\n",
        "class TransformerModel(tf.keras.Model):\n",
        "  #we build up our final model\n",
        "    def __init__(self, num_encoder_layers, num_decoder_layers, hidden_dim, ff_dim, hidden_layer_size, **kwargs):\n",
        "        super(TransformerModel, self).__init__(**kwargs)\n",
        "        self.bert = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "        #this is the pretrained model we use\n",
        "        self.encoder_layers = [TransformerEncoderLayer(hidden_dim, 4, ff_dim) for _ in range(num_encoder_layers)] #we loop through the number of encoders and decoders\n",
        "        # the number depends on our choice\n",
        "        self.decoder_layers = [TransformerDecoderLayer(hidden_dim, 4, ff_dim) for _ in range(num_decoder_layers)]\n",
        "        self.hidden_layer = tf.keras.layers.Dense(hidden_layer_size, activation='relu')\n",
        "        self.output_layer = tf.keras.layers.Dense(1, activation='sigmoid')  #1 hidden layer with certain sizes\n",
        "        #final output layer for classifcation: The final output layer with one cell for binary classification to predict whether two inputs are related or not.\n",
        "    def call(self, inputs, training=False):\n",
        "        bert_outputs = self.bert(inputs)[0]   #call the inputs[0]\n",
        "\n",
        "        enc_output = bert_outputs      #the output of the encoder is the input of the decoder\n",
        "        for encoder in self.encoder_layers:\n",
        "            enc_output = encoder(enc_output, training=training)   #the data are loop through the number of encoders and decoders\n",
        "\n",
        "        dec_output = enc_output\n",
        "        for decoder in self.decoder_layers:\n",
        "            dec_output = decoder(dec_output, enc_output, training=training)\n",
        "\n",
        "        hidden_output = self.hidden_layer(tf.reduce_mean(dec_output, axis=1))  #it pass the hidden layers\n",
        "        output = self.output_layer(hidden_output)  #this return the output\n",
        "        return output\n",
        "\n",
        "# These are the paramters we define\n",
        "\n",
        "\n",
        "hidden_dim = 768  #this is the required numbers\n",
        "ff_dim = 64    #this is the numbers suggested by professor\n",
        "hidden_layer_size = 64\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnwcCZjGFGw9"
      },
      "source": [
        "#### Finding the best hyperparamters\n",
        "\n",
        "- In this part we search for different input of parameters and find the best paramters for encoding and decoding layers\n",
        "\n",
        "\n",
        "- Due to the memory problem,we cannot do too many thing and I just create an example to show this method works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLF5M5W0uQAy"
      },
      "outputs": [],
      "source": [
        "# we train our model\n",
        "\n",
        "\n",
        "\n",
        "# we create our dataset objects with traning ,vdalidation and test_dataset in our model\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs['input_ids'], train_labels)).batch(32)\n",
        "dev_test_dataset = tf.data.Dataset.from_tensor_slices((dev_test_inputs['input_ids'], dev_test_labels)).batch(32)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_inputs['input_ids'], test_labels)).batch(32)\n",
        "\n",
        "def transformer_tuning(num_encoder_layers, num_decoder_layers):\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs['input_ids'], train_labels)).batch(32)\n",
        "    dev_test_dataset = tf.data.Dataset.from_tensor_slices((dev_test_inputs['input_ids'], dev_test_labels)).batch(32)\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices((test_inputs['input_ids'], test_labels)).batch(32)\n",
        "    transformer_model = TransformerModel(num_encoder_layers, num_decoder_layers,768, 64, 64)\n",
        "    transformer_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# we train it with several epoches\n",
        "    epochs = 2\n",
        "    transformer_model.fit(train_dataset, validation_data=dev_test_dataset, epochs=epochs)\n",
        "\n",
        "    return (transformer_model.evaluate(test_dataset)[0],i)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MbT2HKjvGkw",
        "outputId": "14bb1973-9e9f-486b-dafd-5d61ad9a27e5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_8/bert/pooler/dense/kernel:0', 'tf_bert_model_8/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_8/bert/pooler/dense/kernel:0', 'tf_bert_model_8/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_8/bert/pooler/dense/kernel:0', 'tf_bert_model_8/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_8/bert/pooler/dense/kernel:0', 'tf_bert_model_8/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 272s 1s/step - loss: 0.6357 - accuracy: 0.6800 - val_loss: 0.6101 - val_accuracy: 0.6840\n",
            "Epoch 2/2\n",
            "157/157 [==============================] - 182s 1s/step - loss: 0.5930 - accuracy: 0.6966 - val_loss: 0.6112 - val_accuracy: 0.6680\n",
            "16/16 [==============================] - 6s 355ms/step - loss: 0.6222 - accuracy: 0.6660\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_9/bert/pooler/dense/kernel:0', 'tf_bert_model_9/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_9/bert/pooler/dense/kernel:0', 'tf_bert_model_9/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_9/bert/pooler/dense/kernel:0', 'tf_bert_model_9/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_9/bert/pooler/dense/kernel:0', 'tf_bert_model_9/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 301s 1s/step - loss: 0.6324 - accuracy: 0.6796 - val_loss: 0.6107 - val_accuracy: 0.6840\n",
            "Epoch 2/2\n",
            "157/157 [==============================] - 226s 1s/step - loss: 0.6025 - accuracy: 0.6882 - val_loss: 0.6093 - val_accuracy: 0.6820\n",
            "16/16 [==============================] - 7s 462ms/step - loss: 0.6159 - accuracy: 0.6720\n"
          ]
        }
      ],
      "source": [
        "# we have three trial with different hyperparameters\n",
        "score_list=[]                         # We want to see determine best performance of each model\n",
        "for i in [1,2,3]:   #to be simple: I only use the same hyperparamters for each layers # in this task we use 2 layers but it works for 3 layers\n",
        "  score_list.append(transformer_tuning(i,i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-va_b6vyAKI",
        "outputId": "76d1a990-9ec8-425e-c72a-160d3b0ec199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "minimum_lost=100000\n",
        "\n",
        "for i , j in (score_list):   # we search for the best validation minimum losts with the best_praramters\n",
        "  if i<=minimum_lost:\n",
        "    best_praramters=j\n",
        "    minimum_lost=i\n",
        "\n",
        "best_praramters   #this is the best number of it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ajaUna2Fua8"
      },
      "source": [
        "#### we retrain the model using best hyperparameters and make prediction\n",
        "\n",
        "- We get the F1 score using inbuit method\n",
        "\n",
        "-Implement a bb_summariser that returns the  n sentences with highest predicted score.\n",
        "\n",
        " - Our code returns the IDs of the  n  sentences that have the highest prediction score in the given question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BO2Gh-A0vpnu",
        "outputId": "4a377b2c-51bf-416a-ecd0-a0a02059f032"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_10/bert/pooler/dense/kernel:0', 'tf_bert_model_10/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_10/bert/pooler/dense/kernel:0', 'tf_bert_model_10/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_10/bert/pooler/dense/kernel:0', 'tf_bert_model_10/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_10/bert/pooler/dense/kernel:0', 'tf_bert_model_10/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 308s 1s/step - loss: 0.6282 - accuracy: 0.6848 - val_loss: 0.6105 - val_accuracy: 0.6840\n",
            "Epoch 2/2\n",
            "157/157 [==============================] - 223s 1s/step - loss: 0.5973 - accuracy: 0.6916 - val_loss: 0.6076 - val_accuracy: 0.6860\n",
            "16/16 [==============================] - 7s 465ms/step - loss: 0.6188 - accuracy: 0.6800\n",
            "Test Accuracy: 0.6800000071525574\n",
            "16/16 [==============================] - 23s 445ms/step\n",
            "F1 Score: 0.0909090909090909\n"
          ]
        }
      ],
      "source": [
        "# we train our model\n",
        "transformer_model = TransformerModel(best_praramters, best_praramters, hidden_dim, ff_dim, hidden_layer_size)\n",
        "transformer_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# we create our dataset objects with traning ,vdalidation and test_dataset in our model\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs['input_ids'], train_labels)).batch(32)\n",
        "dev_test_dataset = tf.data.Dataset.from_tensor_slices((dev_test_inputs['input_ids'], dev_test_labels)).batch(32)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_inputs['input_ids'], test_labels)).batch(32)\n",
        "\n",
        "# we train it with several epoches\n",
        "epochs = 2\n",
        "transformer_model.fit(train_dataset, validation_data=dev_test_dataset, epochs=epochs)\n",
        "\n",
        "#evaluation\n",
        "test_loss, test_accuracy = transformer_model.evaluate(test_dataset)\n",
        "print(f'Test Accuracy: {test_accuracy}')\n",
        "\n",
        "#we compare our F1 score with a test_prediction thershold\n",
        "test_predictions = transformer_model.predict(test_inputs['input_ids']).flatten()\n",
        "test_predictions = (test_predictions > 0.5).astype(int)   #these booleans value will be our predicted label\n",
        "\n",
        "#we need the ground true labels to compare to true labels in F1 score\n",
        "y_true = np.array(test_labels)\n",
        "#caluclate our F1 score\n",
        "f1 = f1_score(y_true, test_predictions)\n",
        "\n",
        "\n",
        "print(f'F1 Score: {f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiKgUNAK0csa"
      },
      "outputs": [],
      "source": [
        "def bb_summariser(csvfile, questionids, n=1):\n",
        "  qid_list=questionids   # The input questionids is a list of question ids.\n",
        "  test_2 = pd.read_csv(csvfile)   #input file we want\n",
        "  total=[]\n",
        "\n",
        "  data=pd.read_csv(csvfile)\n",
        "  for qid in qid_list:\n",
        "      target=data.loc[(data[\"qid\"]==qid)]\n",
        "\n",
        "      answers_input=target[\"sentence text\"].to_list()\n",
        "      answers_input\n",
        "      answers_encoded= [x+\" \"+\"[SEP]\" for x in answers_input]\n",
        "\n",
        "      question_input=target[\"question\"].to_list()\n",
        "      question_encoded= [\"[CLS]\"+\" \"+x+\" \"+\"[SEP]\"+\" \" for x in question_input]\n",
        "\n",
        "      label=target[\"label\"].to_list()\n",
        "\n",
        "      w=[]\n",
        "      for last, first in zip(question_encoded, answers_encoded):\n",
        "          w.append(last+first)\n",
        "\n",
        "      qid_inputs=BertTokenizer_pairs(w, MAX_LEN)\n",
        "      value=transformer_model.predict(qid_inputs['input_ids']).flatten()\n",
        "\n",
        "      list_=list(np.argsort(value))[::-1]\n",
        "\n",
        "      total.append(list_[0:n])\n",
        "\n",
        "  return total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAuG14ge4ZCN",
        "outputId": "32e96df4-0ad4-4222-b2e9-b11da655c0dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 17s 17s/step\n",
            "1/1 [==============================] - 0s 79ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[[28, 16, 29], [0, 1, 4]]"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bb_summariser(\"test.csv\",[6,4220],3) # return the highest prediction score based on accuracy. #we use some examples only #inputs are list of iD  #output are the top n sentenses gives\n",
        "#best accuracy  #I dont have enough RAM to use all texts #but my codes show it works"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGCuT0srF8EZ"
      },
      "source": [
        "- We plot all results below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MCzLPEJXb0T",
        "outputId": "e596f0fc-7754-4afc-a1cb-0988ca6295bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Our F1 score in transformer is 0.0909090909090909\n"
          ]
        }
      ],
      "source": [
        "f1_score_3=0.0909090909090909\n",
        "print (\"Our F1 score in transformer is\",f1_score_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "w0JxmlJltTBm",
        "outputId": "ef1a99df-5f2b-4617-9f4e-1919a741afce"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# creating the dataset\n",
        "data = {'Simplemodel_1':f1_score_1, 'LSTMCmodel_2':f1_score_2,'Transformer model':f1_score_3}\n",
        "keys = list(data.keys())\n",
        "values = list(data.values())\n",
        "\n",
        "fig = plt.figure(figsize = (10, 5))\n",
        "axes = plt.axes()\n",
        "axes.set_ylim([0.2, 0.7])\n",
        "# creating the bar plot\n",
        "plt.bar(keys, values\n",
        "        )\n",
        "\n",
        "plt.title('F1 for all tasks')\n",
        "plt.xlabel(\"Classifier\")\n",
        "plt.ylabel(\"The F1 of accuracy\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XqjdmRptwCC"
      },
      "source": [
        "### comment of the result:\n",
        "\n",
        "- in this task we have tune our models and from the graph we can see the F1 score is very low.It might due to the fact that we didnt choose a proper thereshold value which causes more time to figure out.\n",
        "\n",
        "- In this task we use fewer data (round thorusand ) and fewer layers due to memory problem. It is clearly the codes are runned sccuessfully however we need to have more exploration of best hyperparamters in future\n",
        "\n",
        "- it seems the transformer is not useful under this construction and I spend lot of hours to run and test it\n",
        "- if we have more time,we can build up a better one\n",
        "\n",
        "- In conclusion: few data and few layers (or didnt get the correct layers) might be the problem.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppkBsuB_0dC9"
      },
      "source": [
        "# Submission\n",
        "\n",
        "Your submission should consist of this Jupyter notebook with all your code and explanations inserted into the notebook as text cells. **The notebook should contain the output of the runs. All code should run. Code with syntax errors or code without output will not be assessed.**\n",
        "\n",
        "**Do not submit multiple files.**\n",
        "\n",
        "Examine the text cells of this notebook so that you can have an idea of how to format text for good visual impact. You can also read this useful [guide to the MarkDown notation](https://daringfireball.net/projects/markdown/syntax),  which explains the format of the text cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_8MeiWq_jHk"
      },
      "source": [
        "- Question 3: Part of the transformer codes are from AI /other source\n",
        "- Question 1 and 2: for the part in building up the custom model(e.g custom training loop) ,i actually try to understand the code from this website:\n",
        "\n",
        "https://keras.io/examples/vision/siamese_network/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKUv_s1I0GSE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "a7b63e7410c98f344f02082f10d790581d1dba1eeb1c8fe30f342f6109f0429e"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}